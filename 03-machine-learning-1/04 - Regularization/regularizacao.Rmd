---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.1.3
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

<!-- #region -->
# Regulariza√ß√£o em Machine Learning

<br>
<img src="img\regularizacao.png" style="height:350px">
<br>

Um dos principais aspectos do treinamento do seu modelo de aprendizado de m√°quina √© evitar o overfitting, pois neste caso o modelo ter√° uma baixa precis√£o. Isso acontece porque o seu modelo dificilmente ir√° conseguir capturar o ru√≠do em seu conjunto de dados de treinamento. Por ru√≠do, queremos dizer os pontos de dados que realmente n√£o representam as propriedades reais de seus dados, mas de chance aleat√≥ria. Aprender esses pontos de dados torna o seu modelo mais flex√≠vel, sob o risco de overfitting.

O conceito de balanceamento de vi√©s e vari√¢ncia √© √∫til para entender o fen√¥meno do overfitting e varia√ß√£o de balanceamento, para controlar erros do Machine Leanrning

#### No mundo do Machine Learning, a precis√£o √© tudo.

Regulariza√ß√£o √© uma t√©cnica que restringe/regulariza/reduz as estimativas dos coeficientes para zero. Em outras palavras, esta t√©cnica desencoraja a aprendizagem de um modelo mais complexo ou flex√≠vel, de modo a evitar o risco de overfitting.

Uma rela√ß√£o simples de regress√£o linear se parece com a f√≥rmula abaixo, onde $y$ representa a rela√ß√£o aprendida e ùúÉ representa as estimativas de coeficiente para diferentes vari√°veis ou preditores (x).


<br>
<br>
$$y = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + ... + \theta_nx_n$$
<br>
<br>

O procedimento de ajuste envolve uma fun√ß√£o de perda, conhecida como soma residual de quadrados ou RSS, sendo que os coeficientes s√£o escolhidos, de forma que minimizem essa fun√ß√£o de perda.

A regulariza√ß√£o ajustar√° os coeficientes com base nos seus dados de treinamento. Se houver ru√≠do nos dados de treinamento, os coeficientes estimados n√£o ser√£o generalizados nos dados futuros. √â aqui que a regulariza√ß√£o entra e encolhe ou regulariza essas estimativas aprendidas para zero.


## Ridge Regression

<br>
<br>
$$Ridge : \lambda \sum_{i=1}^{n} \theta_i^2$$
<br>
<br>

A f√≥rmula acima mostra o coeficiente de Ridge que modifica a regress√£o pela adi√ß√£o da quantidade de contra√ß√£o. Agora, os coeficientes s√£o estimados minimizando essa fun√ß√£o, e o Œª √© o par√¢metro de sintonia que decide quanto queremos penalizar a flexibilidade do nosso modelo. O aumento na flexibilidade de um modelo √© representado pelo aumento de seus coeficientes e, se quisermos minimizar a fun√ß√£o acima, esses coeficientes precisam ser pequenos. √â assim que a t√©cnica de regress√£o de Ridge impede que os coeficientes subam demais. Al√©m disso, observe que encolhemos a associa√ß√£o estimada de cada vari√°vel com a resposta, exceto o intercepto $\theta_0$. 

Quando Œª = 0, o termo de penalidade n√£o tem efeito, e as estimativas produzidas por regress√£o de rebordo ser√£o iguais a m√≠nimos quadrados. No entanto, como Œª ‚Üí ‚àû, o impacto da penalidade de contra√ß√£o aumenta, e as estimativas coe Ô¨Å cientes da regress√£o da crista se aproximam de zero. Como pode ser visto, selecionar um bom valor de Œª √© cr√≠tico. A valida√ß√£o cruzada √© √∫til para esse prop√≥sito. As estimativas dos coeficientes produzidas por este m√©todo s√£o tamb√©m conhecidas como norma L2.

Os coeficientes que s√£o produzidos pelo m√©todo dos m√≠nimos quadrados padr√£o s√£o escala equivariante, ou seja, se multiplicarmos cada entrada por c, ent√£o os coeficientes correspondentes s√£o escalonados por um fator de 1 / c. Portanto, independentemente de como o preditor √© escalado, a multiplica√ß√£o de preditor e coeficiente permanece a mesma. No entanto, esse n√£o √© o caso da regress√£o de Ridge e, portanto, precisamos padronizar os preditores ou trazer os preditores para a mesma escala antes de executar a regress√£o de Ridge.

## Lasso Regression

<br>
<br>
$$Lasso : \lambda \sum_{i=1}^{n} |\theta_i| $$
<br>
<br>

Lasso √© outra varia√ß√£o de regulariza√ß√£o, em que a fun√ß√£o acima √© minimizada. √â claro que essa varia√ß√£o difere da regress√£o de Ridge apenas em penalizar os altos coeficientes. Ele usa | $\theta_i$ | (m√≥dulo) em vez de quadrados de $\theta_i$, como sua penalidade. Nas estat√≠sticas, isso √© conhecido como a norma L1.

Vamos dar uma olhada nos m√©todos acima com uma perspectiva diferente: a regress√£o de Ridge pode ser pensada como resolvendo uma equa√ß√£o, onde a soma dos quadrados dos coeficientes √© menor ou igual a s. E o Lassoo pode ser pensado como uma equa√ß√£o onde a soma do m√≥dulo de coeficientes √© menor ou igual a s. Aqui, s √© uma constante que existe para cada valor do fator de encolhimento Œª. Essas equa√ß√µes tamb√©m s√£o chamadas de fun√ß√µes de restri√ß√£o.

Considere os seus dois par√¢metros em um determinado problema. Ent√£o, de acordo com a formula√ß√£o acima, a regress√£o da crista √© expressa por $\theta_1^2 +  \theta_2^2 ‚â§ s$. Isto implica que os coeficientes de regress√£o da crista t√™m a menor RSS (fun√ß√£o de perda) para todos os pontos que se encontram dentro da circunfer√™ncia dada por $\theta_1^2 +  \theta_2^2 ‚â§ s$.

Da mesma forma, para la√ßos, a equa√ß√£o se torna $ |\theta_1| +  |\theta_2| ‚â§ s $. Isto implica que os coeficientes de la√ßo possuem o menor RSS (fun√ß√£o de perda) para todos os pontos que est√£o dentro do diamante dado por $ |\theta_1| +  |\theta_2| ‚â§ s $.

Artigo original:

## [Regularization in Machine Learning](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)
<!-- #endregion -->

<!-- #region -->
# Regularization - Case

<br>
<img src="img\house_pricing.png" style="height:550px">
<br>

Voc√™ ficou famoso ap√≥s o bom trabalho com as notas do IMDB e aumentou o lucro da companhia utilizando aprendizado de maquina. Agora seu chefe expandiu os neg√≥cios e te chamou para ajuda-lo. Ele teve a seguinte ideia: se podemos otimizar um algoritmo para ganhar dinheiro com um cinema, podemos fazer isso para outros tipos de produtos! Vamos passar a comprar im√≥veis baratos em leil√µes em vende-los pelo pre√ßo justo, e voc√™, o cientista de dados de equipe tem que responder a pergunta:
- Quais im√≥veis est√£o desvalorizados e merecem receber uma proposta?


Para isso disponibilizamos o arquivo com os diversas caracteristicas de v√°rios im√≥veis:
```house_pricing_train.csv```

Nesse problema, utilize 3 regressoes diferentes para treinarmos o conceito de regulariza√ß√£o.


- MQO
- Ridge
- Lasso

Depois utilize o arquivo : ```house_pricing_test.csv``` para escolher os 10 im√≥veis que devemos investir para obter o **maior lucro**.

Dica: Utilize o passo a passo do hands-on do IMDB.
<!-- #endregion -->

```{python}
import pandas as pd
```

```{python}
df_raw = pd.read_csv("house_pricing_train.zip")
```

```{python}
df = df_raw.copy(deep=True)
```

```{python}
df.info()
```

```{python}
df.describe().T['count']
```

```{python}
df.head()
```

```{python}
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
```

```{python}
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso, Ridge
```

```{python}
# matriz de correla√ß√£o entre todas as vari√°veis num√©ricas.
# esse algoritmo s√≥ funciona se n√£o tiver campos nulos.
corrmat = df.corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmax=.8, square=True,annot=True)
```

```{python}
# vamos rodar uma regress√£o com statsmodel para analisar o comportamento e peso das vari√°veis
import statsmodels.formula.api as smf

function1 = '''price ~ 
 + num_bed
 + num_bath
 + size_house
 + size_lot
 + num_floors
 + is_waterfront
 + condition
 + size_basement
 + year_built
 + renovation_date
 + zip
 + latitude
 + longitude
 + avg_size_neighbor_houses
 + avg_size_neighbor_lot'''

model1 = smf.ols(function1, df).fit()
print(model1.summary())

```

```{python}
# uma forma minimamente mais analitica de escolher quais vari√°veis vamos passar log √© olhando
# o terceiro (assimetria) e quarto (curtose) momentos estatisticos e comparar com os valores ideias
# de uma distribui√ß√£o normal (0 e 3 respectivamente).

# Exemplo do calculo da assimetria:
from scipy import stats

numeric_feats = df.dtypes[df.dtypes != "object"].index

skewed_feats = df[numeric_feats].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending=False)
print("\nAssimetria: \n")
skewness = pd.DataFrame({'Skew' :skewed_feats})
skewness.head(20)
```

```{python}
plt.hist(df.size_lot,bins=100, range=(0,300000))
```

```{python}
# vemos por exemplo que a variavel price (nosso target) √© bastante assim√©trica.
fig, ax = plt.subplots()
ax.scatter(x = df['size_lot'], y = df['price'])
plt.ylabel('Price', fontsize=13)
plt.xlabel('Size', fontsize=13)
plt.show()
```

a densidade e a vari√¢ncia √© muito maior na concentra√ß√£o dos pontos (muita vari√¢ncia em uma mesma faixa de dados), ent√£o seria uma boa sa√≠da fazer dois modelos. Na entrada faz um if e sepera a entrada para ir para o respectivo modelo.

```{python}
# Aqui comparamos a distribui√ß√£o real com uma normal te√≥rica com mesma m√©dia e desvio.
sns.distplot(df['price'] , fit=stats.norm);

(mu, sigma) = stats.norm.fit(df['price'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('Price distribution')

fig = plt.figure()
res = stats.probplot(df['price'], plot=plt)
plt.show()
```

```{python}
import numpy as np
```

```{python}
# agora vamos passar uma simples transforma√ß√£o logaritmica e ver se a curva fica mais pr√≥xima
# da normal

fig, ax = plt.subplots()
ax.scatter(x = np.log(df['size_lot']), y = np.log(df['price']))
plt.ylabel('Price', fontsize=13)
plt.xlabel('Size', fontsize=13)
plt.show()


sns.distplot(np.log(df['price']) , fit=stats.norm);

(mu, sigma) = stats.norm.fit(np.log(df['price']))
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('Price distribution')

fig = plt.figure()
res = stats.probplot(np.log(df['price']), plot=plt)
plt.show()
```

com range e dispers√£o muito grandes, fazemos um re-scaling com Logaritmo - perde a interpretabilidade

```{python}
import statsmodels.formula.api as smf

function1 = ''' np.log(price) ~ 
 + num_bed
 + num_bath
 + size_house
 + size_lot
 + num_floors
 + is_waterfront
 + condition
 + size_basement
 + year_built
 + renovation_date
 + zip
 + latitude
 + longitude
 + avg_size_neighbor_houses
 + avg_size_neighbor_lot'''


model1 = smf.ols(function1, df).fit()
print(model1.summary())


```

```{python}
# Aqui alguns exemplos de feature engineering.

# N√£o h√° um manual bem definido para essas opera√ß√µes, por isso conhecer bem como o algortimo
# funciona ajuda-nos a escolher as melhores op√ß√µes.

# Por exemplo, juntar colunas que s√£o muitas parecidas e que as duas tenham pouquissimos dados pode 
# ajudar o algoritmo a "entender mais facilmente". Mas se juntarmos duas colunas diferentes podemos
# atrapalha-lo e ainda pior, se juntarmos uma coluna muito particular achando que ela deveria ser "juntada"
# com outra, podemos perder informac√µes bastante valiosas. Portanto a melhor estrat√©gia √© testar!

g = sns.factorplot(x="num_bath", data=df, kind="count",
                   palette="BuPu", size=6, aspect=1.5)
g.set_xticklabels(step=2)

df['c_num_bath'] = df.num_bath.replace({0:0, 0.50:0, 0.75:0,
                                          1:1, 1.25:1, 1.50:1, 1.75:1,
                                          2:2, 2.25:2, 2.50:2, 2.75:2,
                                          3:3, 3.25:3, 3.50:3, 3.75:3,
                                          4:4, 4.25:4, 4.50:4, 4.75:4,
                                          5:5, 5.25:5, 5.50:5, 5.75:5,
                                          6:5, 6.25:5, 6.50:5, 6.75:5,
                                          7:5, 7.25:5, 7.50:5, 7.75:5,
                                          8:5})

g = sns.factorplot(x="c_num_bath", data=df, kind="count",
                   palette="BuPu", size=6, aspect=1.5)
g.set_xticklabels(step=2)

df['c_num_partial_bath'] = df.num_bath.replace({0:0, 0.25:1, 0.50:2, 0.75:3,
                                                  1:0, 1.25:1, 1.50:2, 1.75:3,
                                                  2:0, 2.25:1, 2.50:2, 2.75:3,
                                                  3:0, 3.25:1, 3.50:2, 3.75:3,
                                                  4:0, 4.25:1, 4.50:2, 4.75:3,
                                                  5:0, 5.25:1, 5.50:2, 5.75:3,
                                                  6:0, 6.25:1, 6.50:2, 6.75:3,
                                                  7:0, 7.25:1, 7.50:2, 7.75:3,
                                                  8:0})

g = sns.factorplot(x="c_num_partial_bath", data=df, kind="count",
                   palette="BuPu", size=6, aspect=1.5)
g.set_xticklabels(step=2)
```

```{python}
avg_zip = df.groupby(df['zip'])[['price', 'size_house', 'size_lot']].median()

avg_zip.rename(columns={'price': 'zip_price',
                        'size_house': 'zip_size_house',
                        'size_lot':'zip_size_lot'}, inplace=True)

avg_zip = avg_zip.reset_index()
df = df.merge(avg_zip, left_on='zip', right_on='zip', how='inner')
```

```{python}
var = ['year_built','renovation_date']
for i in var:
    data = pd.concat([df['price'], df[i]], axis=1)
    f, ax = plt.subplots(figsize=(16, 8))
    fig = sns.boxplot(x=i, y="price", data=data)
    plt.xticks(rotation=90)
```

```{python}
list(df)
```

```{python}
# Criando uma simples fun√ß√£o que cria vari√°veis dummies das colunas escolhidas.
categoricals = ['condition', 'num_floors', 'is_waterfront', 'c_num_bath', 'c_num_partial_bath', 'zip']

def one_hot(df, cols):
    for each in cols:
        dummies = pd.get_dummies(df[each], prefix=each, drop_first=True)
        df = pd.concat([df, dummies], axis=1)
    return df

df = one_hot(df,categoricals)

df = df.drop(categoricals, axis=1)
```

```{python}
df.head()
```

```{python}
# fun√ß√£o para separar nossos dados em treino e teste
from sklearn.model_selection import train_test_split
def dfSplit(df, ratio, y='log_price'):
    train, test = train_test_split(df, test_size = ratio)
    y_train = train[y]
    y_test = test[y]
    x_train = train.ix[:, train.columns != y]
    x_test = test.ix[:, test.columns != y]
    return x_train, y_train, x_test, y_test
```

```{python}
# fun√ß√£o para expandir o polinomio das colunas explicativas (os X`s)
# conforme conversamos em aula, isso serve para que a fun√ß√£o possa entender
# comportamos n√£o lineares por outro lado, se tivermos intera√ß√µes e polinomios sem necessidade
# podemos causar overfiting, ou seja, superajustar nosso modelo.
def tPoly(df, degree=1):
    polynomial = PolynomialFeatures(degree=degree)
    return polynomial.fit_transform(df)
```

```{python}
from sklearn.metrics import make_scorer, mean_squared_error, r2_score
from sklearn import linear_model
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import cross_val_score
```

```{python}
# fun√ß√£o que roda nossos modeelos de machine learning, aplica o polinomio, separa os dados em
# treino e teste, exibe as m√©tricas e plota os resultados.

scorer = make_scorer(mean_squared_error, greater_is_better = False)
def testRegs(df, clf, degree=1, ratio=.2, y='log_price', metrics=[]):

    x_train, y_train, x_test, y_test = dfSplit(df,ratio,y='log_price')

    poly_x_train = tPoly(x_train,degree)
    poly_x_test = tPoly(x_test,degree)

    clf.fit(poly_x_train,y_train)
    
    y_hat = clf.predict(poly_x_test)
    
    rmse = np.sqrt(-cross_val_score(clf, poly_x_train, y_train, scoring = scorer, cv = 10))
    
    print("RMSE:", rmse.mean())
    
    y_train_pred = clf.predict(poly_x_train)
    y_test_pred = clf.predict(poly_x_test)

    
    print('R2 TESTE: %.2f, R2 TREINO: %.2f, Parameters: %i' % (r2_score(y_test, y_hat), 
                                                     clf.score(poly_x_train,y_train), 
                                                     clf.coef_.shape[0]))

#     y_train_pred = y_train_pred.sample(100)
#     y_test_pred = y_test_pred.sample(100)
    
    plt.scatter(y_train_pred, y_train_pred - y_train, c = "blue", marker = "s", label = "Training data")
    plt.scatter(y_test_pred, y_test_pred - y_test, c = "lightgreen", marker = "s", label = "Validation data")
    plt.xlabel("Predicted values")
    plt.ylabel("Residuals")
    plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = "red")
    plt.show()

    plt.scatter(y_train_pred, y_train, c = "blue", marker = "s", label = "Training data")
    plt.scatter(y_test_pred, y_test, c = "lightgreen", marker = "s", label = "Validation data")
    plt.xlabel("Predicted values")
    plt.ylabel("Real values")
    plt.legend(loc = "upper left")
    plt.plot([10.5, 13.5], [10.5, 13.5], c = "red")
    plt.show()
```

```{python}
df['log_price'] = np.log(df['price'])
```

```{python}
del df['price']
```

```{python}
np.exp(13)
```

```{python}
ols_simple = linear_model.LinearRegression()

testRegs(df, ols_simple, 1)
```

```{python}
def print_metrics(y_train, yhat_train,y_test, yhat_test):
    print('\n-----Dados de Treino-----')
    print('MSE - treino', mean_squared_error(y_train, yhat_train))
    print('R2 - treino', r2_score(y_train, yhat_train))

    print('\n-----Dados de Teste-----')
    print('MSE - test', mean_squared_error(y_test, yhat_test))
    print('R2 - test', r2_score(y_test, yhat_test))
```

```{python}
def run_model_with_poly(dataframe, poly_n, modelo, target='log_price'):
        
    poly = PolynomialFeatures(degree=poly_n)

    X = df.drop([target], axis=1)
    
    newY = dataframe[target]
    
    df_temp = pd.DataFrame(poly.fit_transform(X))
    print('\ndf_temp shape', df_temp.shape)
        
    X_train, X_test, y_train, y_test = train_test_split(df_temp,newY,test_size=0.2)

    print('\ntrain shape', X_train.shape)
    print('teste shape', X_test.shape)
    
    modelo.fit(X_train, y_train)

    yhat = modelo.predict(X_train)
    yhat_test = modelo.predict(X_test[list(X_train)])

    print_metrics(y_train, yhat, y_test, yhat_test)
    return modelo
```

```{python}
# # %time run_model_with_poly(df, 2, ols_simple)
```

```{python}
# %time run_model_with_poly(df, 1, ols_simple)
```

```{python}
from sklearn.linear_model import Lasso, Ridge, ElasticNet
```

```{python}
modelo_lasso = Lasso(alpha=.5)

modelo_ridge = Ridge(alpha=.5)
```

```{python}

```

```{python}

```

```{python}

```

```{python}
X = df.drop(['log_price'], axis=1)
y = df['log_price']

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.3)

LISTA_ALPHAS = []
LISTA_R2 = []

for alpha in [0, 0.1, 0.01, 0.001, 0.05]: 
#     alpha = alpha/100
    modelo = Lasso(alpha=alpha)
    modelo.fit(X_train, y_train)
    yhat = modelo.predict(X_test)
    
    LISTA_ALPHAS.append(alpha)
    LISTA_R2.append(r2_score(y_test, yhat))
    
#     print(f'Alpha:{alpha}, R2:{r2_score(y_test, yhat)}')

pd.DataFrame({'alpha':LISTA_ALPHAS, 'r2': LISTA_R2})
```

```{python}

```

```{python}

```

```{python}
reg_lasso = Lasso(alpha=.65)
```

```{python}
# %time run_model_with_poly(df, 2, reg_lasso)
```

```{python}
ridge = linear_model.RidgeCV(alphas = [ 0.01, 0.1, 0.5, 0.75, 1])

# %time run_model_with_poly(df, 2, ridge)
```

```{python}
lasso = linear_model.LassoCV(alphas = [0.01, 0.06, 0.1, 0.3, 0.6, 1], max_iter = 1000, cv = 10)
# %time run_model_with_poly(df, 2, lasso)
```

<!-- #region -->
# Ap√™ndice: dicas de estudo

## Portal Action

Um portal elaborado em conjunto pelas empresas Estatcamp e DigUp, compostas por profissionais capacitados, com mestrado e doutorado em estat√≠stica e computa√ß√£o. Conta atualmente com mais de 3 milh√µes de visualiza√ß√µes ao ano.

[Portal Action](http://www.portalaction.com.br/)

## CsDojo

Learning computer science and coding, made easy.

[csdojo](https://www.csdojo.io/)

## Python

<br>
<img src="img\python.png" style="height:50px">
<br>

### M√©todos

#### [.info()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html)

#### [.head()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html)

#### [.describe()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)

#### [numpy.histogram()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html)

#### [scipy.stats.skew()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skew.html)


### Fun√ß√µes

#### [slicing](https://www.oreilly.com/learning/how-do-i-use-the-slice-notation-in-python)

#### [.apply()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html)

#### [.groupby()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)

#### [aggregation and grouping](https://jakevdp.github.io/PythonDataScienceHandbook/03.08-aggregation-and-grouping.html)

#### [.merge()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)

#### [.get_dummies()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)

#### [.read_csv()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)

## Libraries

#### [missingno](https://github.com/ResidentMario/missingno)

## Dataviz Libraries

#### [matplotlib](https://matplotlib.org/)

#### [matplotlib.pyplot()](https://matplotlib.org/tutorials/introductory/pyplot.html)

#### [seaborn](https://seaborn.pydata.org/)

#### [seaborn.boxplot()](https://seaborn.pydata.org/generated/seaborn.boxplot.html)

#### [seaborn.violinplot()](https://seaborn.pydata.org/generated/seaborn.violinplot.html)






<!-- #endregion -->

```{python}

```
