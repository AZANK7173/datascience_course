{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise exploratória de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependendo do objetivo podemos ter mais ou menos etapas. Vamos escolher um desafio mais dificil para cobrir a maioria das possibilidades de modelagem.\n",
    "\n",
    "No nosso exemplo vamos modelar uma regressão para explicar os preços das casas nos EUA e usa-lo para escolher as casas desvalorizadas.\n",
    "\n",
    "    -> Identificar como a base foi construida\n",
    "    -> Quais foram as regras, essas regras influeciam os dados?\n",
    "    -> Devemos nos preocupar os com outliners?\n",
    "    -> Ao analisar como as variáveis estão distribuidas, temos funções conhecidas?\n",
    "    -> Como as funções se correlacionam, os comportamentos são os previstos?\n",
    "    -> Para estudar a correlação parcial com regressões precisamos mudar a forma dos dados?\n",
    "    -> Transformações logaritmicas\n",
    "    -> Variáveis dummies, quando usa-las\n",
    "    -> Mudando a forma funcional com polinomios\n",
    "    -> Iterando váriaveis\n",
    "    -> Gerando predições"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos Modelando uma regressão multipla para entender como diferentes variáveis afetam os preços de casas nos EUA.\n",
    "\n",
    "Features:\n",
    "- **price** - The last price the house was sold for\n",
    "- **num_bed** - The number of bedrooms\n",
    "- **num_bath** - The number of bathrooms (fractions mean the house has a toilet-only or shower/bathtub-only bathroom)\n",
    "- **size_house** (includes basement) - The size of the house\n",
    "- **size_lot** - The size of the lot\n",
    "- **num_floors** - The number of floors\n",
    "- **is_waterfront** - Whether or not the house is a waterfront house (0 means it is not a waterfront house whereas 1 means that it is a waterfront house)\n",
    "- **condition** - How worn out the house is. Ranges from 1 (needs repairs all over the place) to 5 (the house is very well maintained)\n",
    "- **size_basement** - The size of the basement\n",
    "- **year_built** - The year the house was built\n",
    "- **renovation_date** - The year the house was renovated for the last time. 0 means the house has never been renovated\n",
    "- **zip** - The zip code\n",
    "- **latitude** - Latitude\n",
    "- **longitude** - Longitude\n",
    "- **avg_size_neighbor_houses** - The average house size of the neighbors\n",
    "- **avg_size_neighbor_lot** - The average lot size of the neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando os principais pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import sqlite3\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats='svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando a conexão com o banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(r'../../99 Datasets/datasets.db')\n",
    "query = 'SELECT * FROM house_sales'\n",
    "\n",
    "df = pd.read_sql_query(query, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primeira olhada nas informações e estatísticas descritivas dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando a correlação entre as variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotando um mapa de calor das correlações com todas as variáveis\n",
    "\n",
    "corrmat = df.corr()\n",
    "cols = corrmat.nlargest(10, 'price')['price'].index\n",
    "cm = np.corrcoef(corrmat.values.T)\n",
    "sns.set(font_scale=1.15)\n",
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "hm = sns.heatmap(corrmat, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=corrmat.columns, xticklabels=corrmat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Identificando as 10 variáveis que mais estão mais correlacionadas com o PRICE\n",
    "\n",
    "corrmat = df.corr()\n",
    "cols = corrmat.nlargest(11, 'price')['price'].index\n",
    "cm = np.corrcoef(df[cols].values.T)\n",
    "sns.set(font_scale=1.15)\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando o PAIPLOT para as variáveis mais correlacionadas com um sample (100 amostras)\n",
    "sample = df[cols].sample(100)\n",
    "sns.pairplot(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagem do mapa da região com as casas mais caras em cor mais escura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=r'img\\houses_tableau.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primeiro modelo de regressão para usarmos de benchmark para os próximos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "function1 = '''\n",
    "price ~\n",
    "+ num_bed\n",
    "+ num_bath\n",
    "+ size_house\n",
    "+ size_lot\n",
    "+ num_floors\n",
    "+ is_waterfront\n",
    "+ C(condition)\n",
    "+ size_basement\n",
    "+ year_built\n",
    "+ renovation_date\n",
    "+ zip\n",
    "+ latitude\n",
    "+ longitude\n",
    "+ avg_size_neighbor_houses\n",
    "+ avg_size_neighbor_lot\n",
    "'''\n",
    "\n",
    "model1 = smf.ols(function1, df).fit()\n",
    "print(model1.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando a variável CONDITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition.value_counts(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação de variáveis usando pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renovation Date\n",
    "\n",
    "Ums boa forma de explorar a variável da data de renovação é criar uma DUMMY que indique se a casa foi renovada ou não - para isso, utilizamos o múdulo Numpy.Where, que traz uma condição para a existência de um renovation_date maior que zero como um valor 1 (True), e caso contrário um valor 0 (False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['renovation_date'].value_counts(dropna=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma DUMMY para representar se a casa foi ou não reformada\n",
    "\n",
    "df['dummy_reforma'] = np.where(df[\"renovation_date\"]>0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tempo desde a última reforma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['renovation_date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma outra DUMMY para representar o tempo em anos desde a última reforma\n",
    "\n",
    "df['tempo_ultima_reforma'] = df['renovation_date'].max() - df['renovation_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método mais genérico com uma função e um apply\n",
    "\n",
    "Criando uma função para retornar a subtração do valor máximo pelo ano da data de renovação, e 99 caso a data de renovação seja 0 - depois disso a função é aplicada para a criação de uma nova variável para traduzir o tempo da última reforma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_date = df['renovation_date'].max()\n",
    "\n",
    "def subtrai_coluna_com_se(row):\n",
    "    \n",
    "    if row['renovation_date'] == 0:\n",
    "        return 99\n",
    "    else:\n",
    "        return (max_date - row['renovation_date'])\n",
    "\n",
    "\n",
    "df['tempo_ultima_reforma'] = df.apply(subtrai_coluna_com_se, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tempo_ultima_reforma'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando mais variáveis a partir da data de renovação das casas\n",
    "\n",
    "A intenção é explicar da melhor forma o preço das casas, com baase nos dados existentes - a partir daí o cientista de dados tem que ser criativo e assetrivo nas escolhar das novas variáveis para a criação do melhor modelo para explicação e predição do preço das casas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A variável #renovated? (yes or no) é binária e mostra se ela foi ou nunca foi reformada \n",
    "\n",
    "df[\"renovated?\"] = df[\"renovation_date\"].apply(lambda x: 1 if x!=0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"renovated?\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A variável recent_year coloca nela qual é o ano do imóvel, sendo que considera o ano de renovação caso haja\n",
    "\n",
    "df[\"recent_year\"]=0\n",
    "\n",
    "df[\"recent_year\"] = df.loc[df[\"renovation_date\"]==0,\"year_built\"]\n",
    "\n",
    "df.loc[df[\"renovation_date\"] != 0,\"recent_year\"]=df.loc[df[\"renovation_date\"] != 0 ,\"renovation_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"recent_year\"].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A variável existence_year calcula a diferença entre o ano mais recente da base(2015) e a idade do imóvel (recent_year) \n",
    "\n",
    "df[\"existence_year\"]=2015-df[\"recent_year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"existence_year\"].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop old values\n",
    "\n",
    "df=df.drop(columns=[\"year_built\",\"renovation_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisando novamente as correlações para continuar o trabalho de criação de variáveis e modelagem estatística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT zip, avg(price) as avg_price\n",
    "from dataset_house\n",
    "group by zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma nova variável retirando o último dígito do ZIP das casas\n",
    "\n",
    "Podemos diminuir a dimensão da variável ZIP pegando os primeiros quatro números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['newcep'] = df.zip.astype(str).str[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma variável de tamanho das casas do bairro\n",
    "\n",
    "Para isso agruparemos os ZIPs calculando a média dos tamanhos das casas para cada um, criando uma nova coluna com o tamanho médio. Vamos usar funções no SQL e no pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Identificando os valores dos preços médios por ZIP com uma função FOR\n",
    "\n",
    "for cep in df.zip.unique():\n",
    "    temp = df[df.zip==cep]\n",
    "    print(cep,temp['price'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zip = df.groupby(['zip']).agg({'size_house':'mean'})\n",
    "df_zip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zip.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar o tamanho médio das casas por ZIP devemos fazer um MERGE - código no SQL \n",
    "\n",
    "SELECT *\n",
    "FROM df \n",
    "    inner join df_zip ON df_zip.index = df.zip\n",
    "    \n",
    "    \n",
    "    \n",
    "Seguindo com o código em Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_zip, how='inner', left_on='zip', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma variável para fortalecer o preço das casas que são maiores que a média das casas do bairro\n",
    "\n",
    "Após o MERGE dos DataFrames (original e o df_zip), a coluna size_house passou a ser chamada de size_house_x e o tamanho médio das casas do mesmo bairro passou a ser size_house_y.\n",
    "\n",
    "Tendo a média dos valores da vizinhança podemos criar uma variavel de quanto a casa é maior do que a média das casas da vizinhança e criamos um indice com isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['indice_invejinha'] = df['size_house_x'] / df['size_house_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['indice_invejinha'].plot.hist(bins=30, figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# function1 = '''\n",
    "# price ~ \n",
    "#  + size_house\n",
    "#  + num_bath\n",
    "#  + size_house\n",
    "#  + size_lot\n",
    "#  + num_floors\n",
    "#  + is_waterfront\n",
    "#  + year_built\n",
    "#  + latitude\n",
    "#  + indice_invejinha\n",
    "#  + longitude\n",
    "#  + avg_size_neighbor_houses\n",
    "#  + avg_size_neighbor_lot\n",
    "#  + C(condition)\n",
    "#  + C(zip)\n",
    "# '''\n",
    "\n",
    "# model1 = smf.ols(function1, df).fit()\n",
    "# print(model1.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heterocedasticidade\n",
    "\n",
    "<br>\n",
    "<img src=\"img/heterocedasticidade.png\" width=\"450\" />\n",
    "<br>\n",
    "\n",
    "Heteroscedasticidade ou Heterocedasticidade é o fenômeno estatístico que ocorre quando o modelo de hipótese matemático apresenta variâncias para Y e X(X1, X2, X3,..., Xn) não iguais para todas as observações, contrariando o postulado : $E(u^2)=σ^2; i = 1 , 2 , ⋯ + n $\n",
    "\n",
    "Esta hipótese do Modelo Clássico de Regressão Linear, pressupõe que a variância de cada termo de perturbação $u_i$, condicional aos valores escolhidos das variáveis explicativas, é algum número constante igual a $σ^2$.Ou seja, este postulado é a da homoscedasticidade, ou igual (homo) dispersão (scedasticidade), isto é, igual variância.\n",
    "\n",
    "Em outras palavras, a heterocedasticidade apresenta-se como uma forte dispersão dos dados em torno de uma reta; uma dispersão dos dados perante um modelo econométrico regredido.\n",
    "\n",
    "Uma definição mais precisa seria na qual uma distribuição de frequência em que todas as distribuições condicionadas têm desvios padrão diferentes.\n",
    "\n",
    "O contrário desse fenômeno, a homocedasticidade, se dá pela observância do postulado, isto é, os dados regredidos encontram-se mais homogeneamente e menos dispersos (concentrados) em torno da reta de regressão do modelo.\n",
    "\n",
    "Sua detecção pode ser realizada por meio do Teste de White, que consiste num teste residual. \n",
    "\n",
    "A heteroscedasticidade não elimina as propriedades de inexistência de viés e consistência dos estimadores de MQO, no entanto, eles deixam de ter variância mínima e eficiência, ou seja, não são os melhores estimadores lineares não-viesados (MELNV).\n",
    "\n",
    "As medidas corretivas não são fáceis de serem implementadas. Se a amostra for grande, podemos obter os erros padrão com heteroscedasticidade corrigida segundo White dos estimadores de MQO e realizar inferências estatísticas com base nesses erros padrão. Por exemplo, no software Eviews, esta opção está disponível no menu quick, estimate equation, options e então seleciona-se a opção Heterokedasticity consistent coeficient covariance, White.\n",
    "\n",
    "Diferentemente, se olharmos os resíduos de MQO, podemos levantar hipóteses sobre o provável padrão da heteroscedasticidade e transformar os dados originais de tal forma que não haja heteroscedasticidade nos dados transformados.\n",
    "\n",
    "É comum seu acontecimento quando de pesquisas com dados em corte, ou seção transversal (cross section - observações de dados sobre unidades econômicas de diferentes tamanhos). \n",
    "\n",
    "\n",
    "\n",
    "# Homocedasticidade\n",
    "\n",
    "Na estatística, uma sequência ou um vetor de variáveis aleatórias é homoscedástico se todas as variáveis aleatórias na sequência ou vetor tiverem a mesma variância finita. Isso também é conhecido como homogeneidade de variância. A noção complementar é chamada heterocedasticidade. As grafias homoscedasticidade e heteroscedasticidade também são usadas com frequência.\n",
    "\n",
    "A suposição de homocedasticidade simplifica o tratamento matemático e computacional. Graves violações na homocedasticidade (supondo-se que a distribuição dos dados é homocedástica quando na realidade é heteroscedástica) pode resultar em superestimar a qualidade do ajuste medido pelo coeficiente de Pearson. Exemplo: Salario e poupança\n",
    "\n",
    "## Aplicando os conceitos de Heterocedasticidade ao nosso exemplo\n",
    "\n",
    "Analisando o primeiro scatterplot (preço x tamanho), notamos algumas variaveis com variância não constante, para torna-la homocedastica podemos aplicar diversas técnicas, usaremos o mais simples que é a aplicação de **LOG** nas duas variaveis pois além deconseguimos a variância constante, temos uma interpretação de elasticidade (taxa de variação) para os parâmetros.\n",
    "\n",
    "**Vale ressaltar que a função LOG do Numpy está se referindo à operação de Logarítmo Natural, ou Logarítmo na base *e*, que é o número de Euller**\n",
    "\n",
    "## Número de Euller\n",
    "\n",
    "Na matemática, o número de Euler, denominado em homenagem ao matemático suíço Leonhard Euler, é a base dos logaritmos naturais. As variantes do nome do número incluem: número de Napier, número de Neper, constante de Néper, número neperiano, constante matemática, número exponencial etc. A primeira referência à constante foi publicada em 1618 na tabela de um apêndice de um trabalho sobre logaritmos de John Napier. No entanto, este não contém a constante propriamente dita, mas apenas uma simples lista de logaritmos naturais calculados a partir desta. A primeira indicação da constante foi descoberta por Jakob Bernoulli, quando tentava encontrar um valor para a expressão do cálculo de juros compostos, cujo valor é aproximadamente 2,718281828459045235360287. \n",
    "\n",
    "Abaixo a relação entre Preços e o tamanho da casa que imaginamos que tenha uma forte e positiva correlação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['size_house_x', 'price']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de dispersão do preço das casas pelo seu tamanho\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = df['size_house_x'], y = df['price'])\n",
    "plt.ylabel('Price', fontsize=13)\n",
    "plt.xlabel('Size', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O formato do gráfico em funil é um clássico exemplo de Heterocedasticidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOG: linearização corrige tanto a não linearidade, ameniza outliners e heterocedasticidade. \n",
    "\n",
    "Não necessáriamente precisamos ter distribuições normais para nossas amostras, mas ela ter essa caracteristica permite que façamos analises não só mais eficientes mas principalmente mais robustas já que a maioria dos algoritmos de regressões que usaremos trazem betas significativos para qualquer distribuição apenas com o primeiro momento, mas não somos capazes de fazer testes de hipótese sem o segundo momento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando o gráfico de distribuição dos preços das casas com a aproximação da curva normal\n",
    "\n",
    "sns.distplot(df['price'] , fit=stats.norm);\n",
    "\n",
    "(mu, sigma) = stats.norm.fit(df['price'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Price distribution')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráfico Q-Q\n",
    "\n",
    "<br>\n",
    "<img src=\"img/qqplot.png\" width=\"450\" />\n",
    "<br>\n",
    "\n",
    "*Origem: Wikipédia, a enciclopédia livre.*\n",
    "\n",
    "Um gráfico Q-Q dados exponenciais independentes e randomicamente gerados, (X ~ Exp(1)). Este gráfico Q–Q compara uma amostra de dados no eixo vertical a uma estatística de população no eixo horizontal. Os pontos seguem um forte padrão não linear, sugerindo que os dados não são distribuídos com um padrão normal (X ~ N(0,1)). O deslocamento entre a linha e os pontos sugere que a média dos dados não é 0. A mediana dos pontos pode ser determinada a estar perto de 0,7\n",
    "Gráfico Q-Q normal comparando dados normais independentes gerados aleatoriamente no eixo vertical a uma população normal padrão no eixo horizontal. A linearidade dos pontos sugere que os dados são normalmente distribuídos.\n",
    "\n",
    "Em estatística, um gráfico Q-Q[1] (\"Q\" significa quantil) é um gráfico de probabilidades, que é um método gráfico para comparar duas distribuições de probabilidade, traçando seus quantis uns contra os outros. Primeiro, o conjunto de intervalos para os quantis é escolhido. Um ponto (x, y) no gráfico corresponde a um dos quantis da segunda distribuição (coordenada y) plotadas contra o mesmo mesmo quantil da primeira distribuição de (coordenada x). Portanto, a linha é uma curva paramétrica com o parâmetro que é o (número do) intervalo para quantil.\n",
    "\n",
    "Se as duas distribuições que estão sendo comparadas são semelhantes, os pontos no gráfico Q-Q vai repousar na linha y = x, aproximadamente. Se as distribuições são linearmente relacionadas, os pontos no gráfico Q-Q irão repousar em uma linha, aproximadamente, mas não necessariamente na linha y = x. gráficos Q-Q também podem ser usados como meio gráfico de estimativa de parâmetros de dispersão e tendência central em uma família de distribuições.\n",
    "\n",
    "Um gráfico Q-Q é usado para comparar as formas de distribuições, fornecendo uma exibição gráfica de como as propriedades, tais como medidas de tendência central, dispersão e assimetria são semelhantes ou diferentes nas duas distribuições. gráficos Q-Q podem ser usados para comparar conjuntos de dados ou distribuições teóricas. O uso de gráficos Q-Q para comparação de duas amostras de dados pode ser visto como uma abordagem não-paramétrica para comparação de suas distribuições subjacentes. Um gráfico Q-Q geralmente é uma abordagem mais poderosa para fazer isso do que a técnica comum de comparação de histogramas das duas amostras, mas requer mais habilidade para interpretar. Gráficos Q-Q são comumente usados para comparar um conjunto de dados com um modelo teórico.[2] Isto pode fornecer uma avaliação de \"qualidade de ajuste\" que é gráfica, ao invés de reduzir a uma exibição numérica. gráficos Q-Q também são usados para comparar duas distribuições teóricas entre si. Uma vez que gráficos Q-Q compararam distribuições, não há necessidade para os valores a serem observados como pares, como em um gráfico de dispersão, ou mesmo para o número de valores nos dois grupos sendo comparados ser igual.\n",
    "\n",
    "O termo \"gráfico de probabilidades\" às vezes, refere-se especificamente a um gráfico Q-Q, umas vezes a uma classe gráficos e outras para o menos comumente usado gráfico P-P. O coeficiente de correlação do gráfico de probabilidade é uma grandeza derivada da ideia de gráficos Q-Q, que mede a concordância de uma distribuição ajustada com os dados observados e que às vezes é usada como um meio de ajuste de uma distribuição de dados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando o gráfico de probabilidade dos preços das casas\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df['price'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando log natural aos preços e aos tamanhos das casas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = np.log(df['size_house_x']), y = np.log(df['price']))\n",
    "plt.ylabel('Price', fontsize=13)\n",
    "plt.xlabel('Size', fontsize=13)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.distplot(np.log(df['price']) , fit=stats.norm);\n",
    "\n",
    "(mu, sigma) = stats.norm.fit(np.log(df['price']))\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Price distribution')\n",
    "\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(np.log(df['price']), plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A segunda variável com maior correlação é a média de tamanho das casas da vizinhança.\n",
    "\n",
    "Uma forma alternativa de transformação dessa informação em uma variavel explicativa interessante é considerar não exatamente o tamanho médio das casas do bairro, mas o quanto estamos proximos ou longe da média das casas do bairro, pois a média pode ser grande ou pequena, mas nossa casa pode ser ainda maior ou ainda menor, não fazendo a comparação relativa. \n",
    "\n",
    "Portanto vamos analisar a variável, aplicar a LOG-Linearização e criar uma nova variável que será uma proporção entre a venda e a sua média.\n",
    "\n",
    "Obs: Não faria sentido criar novas variaveis que fossem combinações lineares entre as variaveis do modelo, muitos softwares simplesmente não rodam por não conseguirem inveter a matriz de parametros (X) acusando multicolinearidade perfeita (ou em termos mais economicos, a variavel criada como combinação linear de outras não acrescenta nenhum novo poder explicativo ao modelo), neste casso criaremos um indice percentual para testarmos sua correlação com o preço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando o gráfico de distribuição dos tamanhos médios das casas da vizinhança com a aproximação da curva normal\n",
    "\n",
    "sns.distplot(df['avg_size_neighbor_houses'] , fit=stats.norm);\n",
    "\n",
    "(mu, sigma) = stats.norm.fit(df['avg_size_neighbor_houses'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('avg_size_neighbor_houses')\n",
    "\n",
    "# Gráfico de dispersão dos tamanhos médios das cadas da vizinhança pelo preço\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = df['avg_size_neighbor_houses'], y = df['price'])\n",
    "plt.ylabel('Price', fontsize=13)\n",
    "plt.xlabel('neighbor', fontsize=13)\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de probabilidades dos tamanhos médios das cadas da vizinhança\n",
    "res1 = stats.probplot(df['avg_size_neighbor_houses'], plot=plt)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando log natural aos tamanhos médios das cadas da vizinhança"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de distribuição com LOG aos tamanhos médios das cadas da vizinhança\n",
    "sns.distplot(np.log(df['avg_size_neighbor_houses']) , fit=stats.norm);\n",
    "\n",
    "(mu, sigma) = stats.norm.fit(np.log(df['avg_size_neighbor_houses']))\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('avg_size_neighbor_houses')\n",
    "\n",
    "# Gráfico de dispersão dos tamanhos médios das cadas da vizinhança pelo preço\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = np.log(df['avg_size_neighbor_houses']), y = np.log(df['price']))\n",
    "plt.ylabel('Price', fontsize=13)\n",
    "plt.xlabel('neighbor', fontsize=13)\n",
    "plt.show()\n",
    "\n",
    "#Gráfico de probabilidades dos tamanhos médios das cadas da vizinhança\n",
    "res2 = stats.probplot(np.log(df['avg_size_neighbor_houses']), plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como escolher as variáveis que deveriamos aplicar LOG\n",
    "\n",
    "Uma distribuição normal tem 4 momentos como as outras, mas só precisamos definir os 2 primeiros:\n",
    "\n",
    "- Média \n",
    "- Desvio-Padrão\n",
    "\n",
    "os outros dois momentos, assimetria e curtose, são fixos para qualquer curva normal\n",
    "\n",
    "- Assimetria = 0 \n",
    "- Curtose = 3 \n",
    "\n",
    "Uma forma mais analitica é vermos quais são esses dois parâmetros da nossa distribuição, e caso estejam muito distante do padrão, a variável é uma boa candidata a log-linearização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a assimetria (skew) para as variáveis independentes e colocando em um DataFrame\n",
    "\n",
    "numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n",
    "\n",
    "skewed_feats = df[numeric_feats].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo estas informações, todas as variáveis con assimetria acima de 3 poderiam ser linearizadas pela aplicação do LOG natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variáveis Candidatas a Categóricas\n",
    "\n",
    "## Dummy variable (statistics)\n",
    "\n",
    "<br>\n",
    "<img src=\"img/dummies.png\" width=\"450\" />\n",
    "<br>\n",
    "\n",
    "*Origem: Wikipédia, a enciclopédia livre*\n",
    "\n",
    "Em estatística e econometria, particularmente na análise de regressão, uma variável dummy (também conhecida como variável indicadora, variável de design, codificação de um hot, indicador booleano, variável binária, ou variável qualitativa) é aquela que leva a valor 0 ou 1 para indicar a ausência ou presença de algum efeito categórico que pode ser esperado para mudar o resultado. Variáveis ​​dummy são usadas como dispositivos para classificar os dados em categorias mutuamente exclusivas (como fumante / não fumante, etc.). Por exemplo, na análise econométrica de séries temporais, variáveis ​​dummy podem ser usadas para indicar a ocorrência de guerras ou grandes greves. Uma variável dummy pode, assim, ser considerada como um valor de verdade representado como um valor numérico 0 ou 1 (como às vezes é feito na programação de computadores).\n",
    "\n",
    "Variáveis ​​dummy são variáveis ​​\"proxy\" ou substitutos numéricos para fatos qualitativos em um modelo de regressão. Na análise de regressão, as variáveis ​​dependentes podem ser influenciadas não apenas por variáveis ​​quantitativas (renda, produto, preços, etc.), mas também por variáveis ​​qualitativas (gênero, religião, região geográfica, etc.). Uma variável independente fictícia (também chamada de variável explicativa fictícia) que para algumas observações tem um valor de 0 fará com que o coeficiente dessa variável não tenha nenhum papel em influenciar a variável dependente, enquanto quando o dummy assume um valor 1 seu coeficiente age para alterar a interceptação. Por exemplo, suponha que a associação em um grupo seja uma das variáveis ​​qualitativas relevantes para uma regressão. Se a associação ao grupo receber arbitrariamente o valor de 1, todos os outros receberão o valor 0. Então, o intercepto (o valor da variável dependente se todas as outras variáveis ​​explicativas assumiram hipoteticamente o valor zero) seria o termo constante para membros, mas seria o termo constante mais o coeficiente do manequim de associação no caso dos membros do grupo.\n",
    "\n",
    "Variáveis ​​dummy são usadas freqüentemente na análise de séries temporais com mudança de regime, análise sazonal e aplicações de dados qualitativos. Variáveis ​​dummy estão envolvidas em estudos para previsão econômica, estudos biomédicos, pontuação de crédito, modelagem de resposta, etc. Variáveis ​​dummy podem ser incorporadas em métodos tradicionais de regressão ou paradigmas de modelagem desenvolvidos recentemente.\n",
    "\n",
    "Análise das variáveis categóricas com sugestão de tratamento através da criação de ***DUMMIES*** de forma que sejam melhor interpretadas pelo modelo e representam da melhor forma a composição dos preços das casas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando a variável do número de banheiros das casas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.num_bath.plot.hist(bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.num_bath.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma coluna com o LOG dos preços das casas\n",
    "\n",
    "df['log_price'] = np.log1p(df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparando graficamente a distribuição do número de banheiros das casas pelo preço\n",
    "\n",
    "df.plot.scatter(x='num_bath', y='price')\n",
    "\n",
    "df.plot.scatter(x='num_bath', y='log_price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rodando regressões do número de banheiros das casas pelo preço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressão simples do número de banheiros pelo preço das casas\n",
    "\n",
    "function2 = '''\n",
    "price ~ num_bath\n",
    "'''\n",
    "\n",
    "model2 = smf.ols(function2, df).fit()\n",
    "print(model2.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressão simples do número de banheiros pelo LOG do preço das casas\n",
    "\n",
    "function2 = '''\n",
    "np.log1p(price) ~ num_bath\n",
    "'''\n",
    "\n",
    "model2 = smf.ols(function2, df).fit()\n",
    "print(model2.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rodando uma regressão com o número de banheiros como DUMMIES pelo preço das casas\n",
    "\n",
    "function2 = '''\n",
    "price ~ C(num_bath)\n",
    "'''\n",
    "\n",
    "model2 = smf.ols(function2, df).fit()\n",
    "print(model2.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando as correlações entre os preços, LOG de preços e número de banheiros\n",
    "\n",
    "df[['price', 'log_price', 'num_bath']].corr().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando variáveis com as CATEGORIAS para número de banheiros\n",
    "\n",
    "Serão atribuidos valores inteiros para a criação de novas variáveis que representam novas CATEGORIAS para número de banheiros e número parcial de banheiros, com limitação para os maiores valores pequeno número de incidências.\n",
    "\n",
    "O objetivo é obter distribuições que sejam parecidas com a Normal, além de eliminar outliers e dessa forma possam melhor representar a composição dos preços das casas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando o gráfico de distribuição do número de banheiros\n",
    "g = sns.factorplot(x=\"num_bath\", data=df, kind=\"count\",\n",
    "                   palette=\"rainbow\", size=6, aspect=1.5)\n",
    "g.set_xticklabels(step=2)\n",
    "\n",
    "\n",
    "# Criando uma nova DUMMY para o número de banheiros e plotando a distribuição\n",
    "df['c_num_bath'] = df.num_bath.replace({0:0, 0.50:0, 0.75:0,\n",
    "                                          1:1, 1.25:1, 1.50:1, 1.75:1,\n",
    "                                          2:2, 2.25:2, 2.50:2, 2.75:2,\n",
    "                                          3:3, 3.25:3, 3.50:3, 3.75:3,\n",
    "                                          4:4, 4.25:4, 4.50:4, 4.75:4,\n",
    "                                          5:5, 5.25:5, 5.50:5, 5.75:5,\n",
    "                                          6:5, 6.25:5, 6.50:5, 6.75:5,\n",
    "                                          7:5, 7.25:5, 7.50:5, 7.75:5,\n",
    "                                          8:5})\n",
    "\n",
    "g = sns.factorplot(x=\"c_num_bath\", data=df, kind=\"count\",\n",
    "                   palette=\"rainbow\", size=6, aspect=1.5)\n",
    "g.set_xticklabels(step=2)\n",
    "\n",
    "\n",
    "# Criando uma nova DUMMY para o número de banheiros PARCIAIS e plotando a deitribuição\n",
    "df['c_num_partial_bath'] = df.num_bath.replace({0:0, 0.25:1, 0.50:2, 0.75:3,\n",
    "                                                  1:0, 1.25:1, 1.50:2, 1.75:3,\n",
    "                                                  2:0, 2.25:1, 2.50:2, 2.75:3,\n",
    "                                                  3:0, 3.25:1, 3.50:2, 3.75:3,\n",
    "                                                  4:0, 4.25:1, 4.50:2, 4.75:3,\n",
    "                                                  5:0, 5.25:1, 5.50:2, 5.75:3,\n",
    "                                                  6:0, 6.25:1, 6.50:2, 6.75:3,\n",
    "                                                  7:0, 7.25:1, 7.50:2, 7.75:3,\n",
    "                                                  8:0})\n",
    "\n",
    "g = sns.factorplot(x=\"c_num_partial_bath\", data=df, kind=\"count\",\n",
    "                   palette=\"rainbow\", size=6, aspect=1.5)\n",
    "\n",
    "g.set_xticklabels(step=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rodando a regressão com as novas variáveis para o número de banheiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function2 = '''\n",
    "np.log1p(price) ~ C(c_num_bath) + C(c_num_partial_bath) \n",
    "'''\n",
    "\n",
    "model2 = smf.ols(function2, df).fit()\n",
    "print(model2.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando as variáveis de número de quartos\n",
    "\n",
    "Da mesma forma que no número de banheiros, vamos criar uma **DUMMY** para o número de quantos que possa explicar melhor o preço das casas segundo o númeor de quartos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.num_bed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando um gráfico de distribuição para o número de quartos\n",
    "g = sns.factorplot(x=\"num_bed\", data=df, kind=\"count\",\n",
    "                   palette=\"rainbow\", size=6, aspect=1.5)\n",
    "g.set_xticklabels(step=2)\n",
    "\n",
    "\n",
    "# Criando uma DUMMY para o número de quartos que explique melhor o preço das casas\n",
    "df['c_num_bed'] = df.num_bed.replace({ 0:1, 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:6, 8:6, 9:6, 10:6, 33:6})\n",
    "\n",
    "g = sns.factorplot(x=\"c_num_bed\", data=df, kind=\"count\",\n",
    "                   palette=\"rainbow\", size=6, aspect=1.5)\n",
    "g.set_xticklabels(step=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudando a forma funcional das variáveis para capturar não-lineariedades\n",
    "\n",
    "Vamos fazer uma função para a criação de colunas QUADRÁTICAS para cada uma das colunas numéricas do DataFrame - dessa forma podemos capturar não linearidades para as variações das colunas existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coluna in list(df):\n",
    "    try:\n",
    "        df['q_'+str(coluna)] = df[coluna]**2\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()['log_price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefinindo o tamanho da casa\n",
    "\n",
    "df['q_size_house'] = df['size_house_x'] * df['size_house_x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rodando a regressão com as novas variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function3 = ''' log_price ~ num_bed\n",
    "+ num_bath\n",
    "+ size_house_x\n",
    "+ size_lot\n",
    "+ num_floors\n",
    "+ is_waterfront\n",
    "+ condition\n",
    "+ size_basement\n",
    "+ zip\n",
    "+ latitude\n",
    "+ longitude\n",
    "+ avg_size_neighbor_houses\n",
    "+ avg_size_neighbor_lot\n",
    "+ dummy_reforma\n",
    "+ tempo_ultima_reforma\n",
    "+ recent_year\n",
    "+ existence_year\n",
    "+ newcep\n",
    "+ size_house_y\n",
    "+ indice_invejinha\n",
    "+ c_num_bath\n",
    "+ c_num_partial_bath\n",
    "+ c_num_bed\n",
    "+ q_size_house\n",
    "\n",
    "+ q_num_bed\n",
    "+ q_num_bath\n",
    "+ q_size_house_x\n",
    "+ q_size_lot\n",
    "+ q_num_floors\n",
    "+ q_is_waterfront\n",
    "+ q_condition\n",
    "+ q_size_basement\n",
    "+ q_zip\n",
    "+ C(zip)\n",
    "+ q_latitude\n",
    "+ q_longitude\n",
    "+ q_avg_size_neighbor_houses\n",
    "+ q_avg_size_neighbor_lot\n",
    "+ q_dummy_reforma\n",
    "+ q_tempo_ultima_reforma\n",
    "+ q_recent_year\n",
    "+ q_existence_year\n",
    "+ q_size_house_y\n",
    "+ q_indice_invejinha\n",
    "+ q_c_num_bath\n",
    "+ q_c_num_partial_bath\n",
    "+ q_c_num_bed\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model3 = smf.ols(function3, df).fit()\n",
    "print(model3.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando a influência do tamanho das casas na composição do preço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function4 = '''\n",
    "np.log1p(price) ~ size_house_x + q_size_house\n",
    "'''\n",
    "\n",
    "model4 = smf.ols(function4, df).fit()\n",
    "print(model4.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando as correlações com o LOG dos preços"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "corr.sort_values([\"log_price\"], ascending = False, inplace = True)\n",
    "print(corr.log_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizando Predições\n",
    "\n",
    "Agora vamso utilizaro o modelo OLS (Ordinary Least Squares) utilizado anteriormente para análise, para fazer predições dos preços das casas, e deppos analisar estes preços preditos e principalmente como o erro gerado (Valor Real menos o Valor Predito) se comporta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function5 = ''' price ~ num_bed\n",
    "+ num_bath\n",
    "+ size_house_x\n",
    "+ size_lot\n",
    "+ num_floors\n",
    "'''\n",
    "\n",
    "model5 = smf.ols(function5, df).fit()\n",
    "print(model5.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['yhat'] = model5.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['yhat', 'price']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(model4.rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['erro'] = df['yhat'] - df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "mean_squared_error(df.price, df.yhat), r2_score(df.price, df.yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisando o erro\n",
    "\n",
    "\"O dinheiro está no residuo\" M. Silva - isto significa que as melhores oportunidades estarão nas casas com menor preço real de venda mas com mais alto valor predito.\n",
    "\n",
    "Além disso, temos que lembrar que o erro deve ter distribuição normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['erro'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['erro'].plot.hist(bins=40, xlim = (-1000000, 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['erro'].plot.kde(xlim = (-1000000, 1000000) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizando o Erro\n",
    "\n",
    "Dividindo os valores de erro pelo desvio padrão do próprio erro faz o que chamamos de Normalização dos valores do erro, para entendermos quantos desvios padrão estamos afastados do valor médio. Para os erros, o valor médio sempre será zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['erro_em_desvios'] = df['erro']/df['erro'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['erro_em_desvios'].plot.kde(xlim = (-8, 6) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removendo outliners da predição usando probabilidade\n",
    "\n",
    "Podemos identificar que temos muitos valores extremos na distribuição dos erros, o que causa muita imprecisão. A estratégia de tratamento é a remoção destes Outliers. Vamos começar criando uma máscara para removeer os outliers com dois desvios-padrão de distância, e identificar quantos valores serão removidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mascara_prob = (df['erro_em_desvios']<1.96) & (df['erro_em_desvios']>-1.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~mascara_prob].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[mascara_prob].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[mascara_prob].shape[0]/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A remoção de valores com dois desvios-padrão da média parece muito agressivo, pois estaremos eliminando cerca de 4% da amostra. Isso pode enviesar nossos dados. Por isso, vamos identificar quantos valores removemos se adoratmos uma estratégia de 6 sigma de nível de confiança."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mascara_raridade_9999 = (df['erro_em_desvios']<6) & (df['erro_em_desvios']>-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~mascara_raridade_9999].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta estratégia 6 sigma nos permite retirar somente 48 valores extremos do modelo - agora sim seguimos em frente com a nova regressão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function6 = ''' price ~ num_bed\n",
    "+ num_bath\n",
    "+ size_house_x\n",
    "+ size_lot\n",
    "+ num_floors\n",
    "'''\n",
    "\n",
    "model6 = smf.ols(function6, df[mascara_raridade_9999]).fit()\n",
    "print(model6.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisando a DUMMY número de banheiros através de uma regressão\n",
    "\n",
    "Podemos observar cada linha vermelha representando os valores preditos para cada número de banheiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function7 = ''' price ~ size_house_x + C(c_num_bath)\n",
    "'''\n",
    "\n",
    "model7 = smf.ols(function7, df).fit()\n",
    "print(model7.summary2())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (9,9)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig = sm.graphics.plot_fit(model7, 6, ax=ax) # o parâmetro 6 é o índice da variável do modelo (size_house_x)\n",
    "ax.set_ylabel(\"price\")\n",
    "ax.set_xlabel(\"size\")\n",
    "ax.set_title(\"Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acicionando ITERAÇÃO entre as variáveis parar capturar diferenças nas taxas de retorno\n",
    "\n",
    "Vamos iteragir as varáveis is_waterfront com o soze_house_x com multiplicação para geraar novas predições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.is_waterfront.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['size_x_waterfront'] = df['size_house_x'] * df['is_waterfront']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function8 = ''' log_price ~ size_house_x + is_waterfront + size_x_waterfront'''\n",
    "\n",
    "model8 = smf.ols(function8, df).fit()\n",
    "print(model8.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig = sm.graphics.plot_fit(model8, 1, ax=ax)\n",
    "ax.set_ylabel(\"price\")\n",
    "ax.set_xlabel(\"size\")\n",
    "ax.set_title(\"Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rodando o mesmo gréfico para o modelo 5, com parâmetro 5 (size_house_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig = sm.graphics.plot_fit(model6, 3, ax=ax)\n",
    "ax.set_ylabel(\"price\")\n",
    "ax.set_xlabel(\"size\")\n",
    "ax.set_title(\"Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rodando o mesmo gráfico para o modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sm.graphics.plot_fit(model2, 2, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste de normalidade do erro para o Modelo 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.resid.plot.hist(figsize=(12,8), bins=100, xlim=(-1000000,1500000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list(zip(model1.predict(),df['price'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results, columns=['yhat', 'y']).plot.scatter(x='yhat', y='y', figsize=(12,8), xlim=(0,2000000), ylim=(0,2000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results, columns=['yhat', 'y'])\n",
    "results['residuo'] = model1.resid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na análise de resíduo, diferença entre o valor predito e o valor y teste, nota-se que ela está distribuida aleatoriamente, indicando distribuição normal dos resultados e assim, pode-se dizer que o modelo possui poder explicativo satisfatório. No entanto, o teste de Shapiro rejeita a hipótese de normalidade dos resíduos. Dessa forma, ainda que o modelo apresente pontuação razoavelmente alto de 84~85, ainda há pontos para serem melhorados afim de melhorar a distribuicao residual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize'] = (9,9)\n",
    "\n",
    "results[\"residuo2\"] = results[\"y\"] - results[\"yhat\"]\n",
    "results.plot(x = \"yhat\", y = \"residuo\",kind = \"scatter\")\n",
    "\n",
    "t,p=stats.shapiro(results[\"residuo\"])\n",
    "\n",
    "print(\"The Shapiro-test of normality for residuals is {:03.3f} and p-value of {:03.3f}\".format(t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.shapiro(results[\"residuo2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
