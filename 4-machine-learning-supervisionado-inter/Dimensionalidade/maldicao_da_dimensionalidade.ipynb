{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PRACTICA_GUIADA_Maldición de la Dimensionalidad_pt_br.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"4Nx7hNWchk4f","colab_type":"text"},"cell_type":"markdown","source":["# Prática Guiada: Maldição da Dimensionalidade"]},{"metadata":{"id":"F0ue4crmhk4h","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["%pylab inline\n","import seaborn as sns\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KHQFshufhk4m","colab_type":"text"},"cell_type":"markdown","source":["## Proporção de outliers\n","\n","Se temos uma variável distribuída uniformemente em um hipercubo de $d$ dimensões, qual proporção de outliers encontramos? \n","\n","Podemos definir os outliers como aqueles pontos que recebem valores extremos em algumas das $d$ dimensões. \n","\n","Por exemplo, se pensamos em duas dimensões, forma-se um quadrado."]},{"metadata":{"id":"0a8o8E8vhk4n","colab_type":"text"},"cell_type":"markdown","source":["**Obs.:** Este código serve para gerar um conjunto de dados artificial. Não é fundamental entendê-lo em profundidade."]},{"metadata":{"id":"DFKdO1bkhk4n","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["m = 500\n","xs = uniform(0.0,1.0,m)\n","ys = uniform(0.0,1.0,m)\n","X = c_[xs,ys]\n","plot(xs,ys,'o',ms=2);\n","x = [0.0,1.0]\n","p = 0.01 # 1%\n","fill_between(x,0,p,alpha=0.2,color='k')\n","fill_between(x,1-p,1,alpha=0.2,color='k')\n","fill_betweenx(x,0,p,alpha=0.2,color='k')\n","fill_betweenx(x,1-p,1,alpha=0.2,color='k',label='Região de outliers');\n","legend();\n","print('Outliers ',sum([any([(d &lt; .01 or d > .99) for d in p]) for p in X]),'de',m)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RIpxRIK-hk4r","colab_type":"text"},"cell_type":"markdown","source":["Mas o que acontece à medida que aumentamos a dimensionalidade do conjunto de dados? Como não podemos fazer gráficos com mais de três dimensões, o que vamos fazer é um gráfico da evolução da proporção de outliers para cada nível de dimensionalidade."]},{"metadata":{"id":"QJMw7iuvhk4t","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def sample_(d, N=1000):\n","    ‘‘‘Gera uma amostra de 1000 pontos em d dimensões’’’\n","    return [[uniform(0., 1.) for i in range(d)] for _ in range(N)]\n","\n","def corner_count(points):\n","    '''Conta a quantidade de pontos que são outliers em alguma das d dimensões.'''\n","    return mean([any([(d &lt; .01 or d > .99) for d in p]) for p in points])\n","\n","Ds=arange(1,200)\n","plot(Ds, array([corner_count(sample_(d)) for d in Ds])*100);\n","xlabel('Proporção')\n","ylabel('Dimensões');"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hyP_RbGhhk4w","colab_type":"text"},"cell_type":"markdown","source":["* O que vocês observam?\n","* O que acontece com a proporção de outliers à medida que o espaço de previsores aumenta em dimensionalidade?\n","* Qual pode ser a causa disso?"]},{"metadata":{"id":"hWL5px_Fhk4x","colab_type":"text"},"cell_type":"markdown","source":["## Vamos retomar o exemplo de regressão logística\n","\n","A seguir, vamos ver o que acontece com a performance de uma regressão logística sem regularização à medida que a quantidade de dimensões aumenta.\n","\n","A regressão deverá prever dados simulados por meio de distribuições normais multivariadas.\n","\n","* A classe 0 terá média 0 e as variâncias de todas as dimensões valerão 0,5\n","* A classe 1 terá média 1 e as variâncias de todas as dimensões valerão 0,5"]},{"metadata":{"id":"3H7d4iLahk4z","colab_type":"text"},"cell_type":"markdown","source":["### Regressão logística sobre dados de diferentes dimensões\n","\n","Geramos una lista de possíveis dimensões e para cada um desses valores geramos um conjunto de dados simulado com as características descritas acima.\n","\n","Sobre esses dados rodamos e avaliamos uma regressão logística para cada dimensionalidade. A métrica de avaliação será a média do score de teste, utilizando cross validation com uma partição de 5 folds em todos os casos. "]},{"metadata":{"id":"e7l3fm54hk4z","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from sklearn.model_selection import cross_val_score, KFold\n","from sklearn.linear_model import LogisticRegression\n","score = []\n","# Avaliamos o modelo em todas essas dimensões\n","ds = [1,3,6,40,50,60,120,180]\n","\n","for d in ds:\n","\n","    size0 = 40; size1 = 40;\n","    \n","    loc0 = ones(d); loc1 = zeros(d)\n","    \n","    sigma0 = 0.5; sigma1 = 0.5; C0 = sigma0*diag(ones(d)); C1 = sigma1*diag(ones(d))\n","\n","    #Geramos dados multivariados\n","    x = c_[multivariate_normal(loc0,C0,size0),zeros(size0)]\n","    y = c_[multivariate_normal(loc1,C1,size1),ones(size1)]\n","    X = r_[x,y]\n","\n","    # Criamos a matriz de features\n","    dfX = pd.DataFrame(X)\n","    dfX.columns = ['dim'+str(i) for i in range(d)]+['class']\n","    X = dfX.drop('class',axis=1)\n","    y = dfX['class']\n","    \n","    # Salvamos a média do score de cross-validation\n","    model = LogisticRegression(C = 1e10,n_jobs=4)\n","    kf = KFold(5, shuffle=True, random_state=0)\n","    score.append(np.mean(cross_val_score(model,X,y,cv=kf)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"resjF5S9hk42","colab_type":"text"},"cell_type":"markdown","source":["Observamos que a partir de certa quantidade de dimensões, a performance começa a cair muito por causa do excesso de dimensões em relação com a quantidade de dados disponíveis"]},{"metadata":{"id":"9ksFTFFYhk44","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["score"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xYUibY2ihk48","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["plt.plot(ds,score,'o-');\n","ylabel('Accuracy')\n","xlabel('Dimensions');"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k0aSAKsohk5A","colab_type":"text"},"cell_type":"markdown","source":["Se o volume de dados aumentar, a quantidade ótima de dimensões também vai aumentar. "]},{"metadata":{"id":"qKaMQ7rGhk5B","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["score = []\n","# Avaliamos o modelo em todas essas dimensões\n","ds = [1,3,6,40,50,60,120,180]\n","\n","for d in ds:\n","\n","    size0 = 4000; size1 = 4000;\n","    \n","    loc0 = ones(d); loc1 = zeros(d)\n","    \n","    sigma0 = 0.5; sigma1 = 0.5; C0 = sigma0*diag(ones(d)); C1 = sigma1*diag(ones(d))\n","\n","    #Geramos dados multivariados\n","    x = c_[multivariate_normal(loc0,C0,size0),zeros(size0)]\n","    y = c_[multivariate_normal(loc1,C1,size1),ones(size1)]\n","    X = r_[x,y]\n","\n","    # Criamos a matriz de features\n","    dfX = pd.DataFrame(X)\n","    dfX.columns = ['dim'+str(i) for i in range(d)]+['class']\n","    X = dfX.drop('class',axis=1)\n","    y = dfX['class']\n","    \n","    # Salvamos a média do score de cross-validation\n","    model = LogisticRegression(C = 1e10,n_jobs=4)\n","    kf = KFold(5, shuffle=True, random_state=0)\n","    score.append(np.mean(cross_val_score(model,X,y,cv=kf)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I8iuQsaLhk5G","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["plt.plot(ds,score,'o-');\n","ylabel('Accuracy')\n","xlabel('Dimensions');"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ML-OUgYqhk5J","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}