{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Práctica_Guiada_Naive_Bayes_pt_br.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"8sMVCVSZh7jf","colab_type":"text"},"cell_type":"markdown","source":["# PRÁTICA GUIADA: Naive Bayes\n","\n","## Introdução\n","\n","Nesta prática vamos implementar um classificador do tipo Naive Bayes usando a biblioteca scikit-learn."]},{"metadata":{"id":"bpI8sXU_h7ji","colab_type":"text"},"cell_type":"markdown","source":["Cada parâmetro de NaiveBayes representa a probabilidade de pertencer a certa classe com determinado valor da feature $X_{k}$. \n","No caso da classificação de um texto, o que é medido é a probabilidade de o texto pertencer a determinada classe dependendo da aparição ou não de determinada palavra. O produto de todas essas probabilidades é a probabilidade final de pertencer à classe.  \n","\n","O que diferencia cada algoritmo de Naive Bayes é a distribuição adotada por eles para o processo de geração de cada uma das classes. Em sklearn temos:\n","1. GaussianNaiveBayes\n","2. MultinomialNaiveBayes\n","3. BernoulliNaiveBayes\n"]},{"metadata":{"id":"Hm8hoQpQh7ji","colab_type":"text"},"cell_type":"markdown","source":["## Gaussian Naive Bayes\n","\n","Geramos classes com uma distribuição gaussiana. \n"]},{"metadata":{"id":"B5GBnkxih7jk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","from sklearn.datasets import make_blobs\n","\n","\n","X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uIGS3orah7jo","colab_type":"text"},"cell_type":"markdown","source":["Vamos aplicar a este conjunto de dados um modelo do tipo Gaussian Naive Bayes."]},{"metadata":{"id":"5_O5Vzcyh7jp","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Ajustamos o modelo com os dados gerados\n","from sklearn.naive_bayes import GaussianNB\n","\n","model = GaussianNB()\n","model.fit(X, y);"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gwjsPZYEh7jt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# -6 e -14 são os limites inferiores dos dados gerados pela função make_blobs\n","# 14 e 18 são os intervalos ocupados por cada uma das variáveis geradas\n","# Queremos gerar um espaço de pontos que abranja toda a área dos dados originais.\n","\n","rng = np.random.RandomState(0)\n","X_nueva = [-6, -14] + [14, 18] * rng.rand(2000, 2)\n","y_predicha = model.predict(X_nueva)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IDc7KD9-h7jw","colab_type":"text"},"cell_type":"markdown","source":["Depois, vemos a classificação definida pelo modelo sobre todo o espaço de dados, descrito pelos 2000 puntos gerados acima. \n"]},{"metadata":{"id":"nfNOu04qh7jy","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n","lim = plt.axis()\n","plt.scatter(X_nueva[:, 0], X_nueva[:, 1], c=y_predicha, s=50, cmap='RdBu', alpha=0.1)\n","plt.axis(lim);"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2mxXVeVXh7j1","colab_type":"text"},"cell_type":"markdown","source":["No gráfico anterior observa-se o “decision boundary” do modelo surgido da hipótese de uma distribuição gaussiana sobre cada uma das classes nos dados de treinamento. <br>\n","\n","### Cálculo de probabilidades\n","\n","Uma das vantagens deste modelo é que ele nos permite obter a probabilidade de pertença a uma classe ou outra para cada ponto do domínio."]},{"metadata":{"id":"YovT2Lfgh7j2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["yprob = model.predict_proba(X_nueva)\n","yprob[-8:].round(2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BeVaKqcxh7j6","colab_type":"text"},"cell_type":"markdown","source":["\n","## Classificação de texto\n","\n","Para o seguinte exemplo, vamos baixar o conjunto de dados 20 newsgroup, que contém e-mails com pedidos de informações a diferentes empresas de mídia digital sobre diferentes assuntos.\n","\n","Para mais informações sobre o conjunto de dados, consultar: http://qwone.com/~jason/20Newsgroups/\n","\n","### Importamos os dados"]},{"metadata":{"id":"xuLRr0Kgh7j7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from sklearn.datasets import fetch_20newsgroups\n","\n","mis_datos = fetch_20newsgroups()\n","type(mis_datos)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QKbaQG61h7j-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Exploramos o objeto e vemos que o dataframe não tem nomes de colunas\n","mis_datos.keys()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2tc4vAJvh7kE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(type(mis_datos.data),'El corpus de textos es una lista')\n","print(type(mis_datos.data[0]),'Cada elemento de la lista es un string')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EDoJ8MNrh7kG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Vejamos o primeiro texto do corpus\n","print(mis_datos.data[0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BnOfNlfih7kK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Vejamos agora em quais classes os dados são divididos\n","\n","mis_datos.target_names"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZFpTSUfdh7kN","colab_type":"text"},"cell_type":"markdown","source":["#### Split train-test e simplificação do dataframe\n","\n","Para facilitar o problema de classificação, vamos ficar só com algumas das categorias que contêm os dados. A classe Bunch tem um método especialmente arquitetado para isso:\n"]},{"metadata":{"id":"gHlmH4uih7kP","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"collapsed":true},"cell_type":"code","source":["categories = ['talk.religion.misc', 'soc.religion.christian',\n","'sci.space', 'comp.graphics']\n","train = fetch_20newsgroups(subset='train', categories=categories)\n","test = fetch_20newsgroups(subset='test', categories=categories)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bVsG3Tm1h7kR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["len(train.data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b3bAGdnnh7kU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["len(test.data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IAYwOOuXh7kY","colab_type":"text"},"cell_type":"markdown","source":["#### Feature Engineering\n","\n","Para poder implementar um modelo de previsão, precisamos transformar cada um dos documentos de texto pertencentes ao corpus em uma matriz de features.\n","\n","Para isso, vamos utilizar o algoritmo de scikit-learn `TfidfVectorizer()`. Ele tenta vetorizar um corpus de textos contando a ocorrência de cada palavra em cada documento. Primeiro, procuram-se todas as palavras existentes no corpus, e depois é computada a quantidade de aparições em cada documento. Esse processo é denominado \"word count\".\n","\n","O problema dessa abordagem é a existência de palavras que aparecem com muita frequência em um texto, mas contribuem com pouco valor para a classificação: projeta-se que palavras como “a”, “o” ou “do” apareçam muitas vezes em todos os documentos. \n","\n","O que o TfidfVectorizer() faz é calcular a frequência de aparição de um termo em um documento ponderada segundo a aparição no restante do corpus a fim dar mais peso aos termos que diferenciam cada documento dos outros.\n","\n","Mais informações em http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n","e sobre os detalhes de implementação em: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"]},{"metadata":{"id":"PmobS6RRh7kZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"collapsed":true},"cell_type":"code","source":["# Geramos o modelo e o aplicamos aos dados de treinamento\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","modelo_tfidf = TfidfVectorizer()\n","X_train = modelo_tfidf.fit_transform(train.data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9yV2d5oZh7ke","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Vejamos a transformação feita sobre os dados\n","print(len(train.data))\n","print(X_train.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q3b0hymIh7kh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Vejamos que tipo de dados X contém\n","type(X_train)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZWcmDG1Gh7kk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# A matriz dispersa conserva unicamente os valores diferentes de 0.\n","X_train.size"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RYnW3m_zh7kp","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Quando transformamos a matriz em array, a quantidade de elementos é multiplicada várias vezes\n","X_train.toarray().size"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DEmfwy8bh7ks","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Vejamos as primeiras e últimas features construídas pelo modelo\n","modelo_tfidf.get_feature_names()[0:30]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Q9T_xp2bh7kv","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["modelo_tfidf.get_feature_names()[-30:]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JRiGh1dbh7k0","colab_type":"text"},"cell_type":"markdown","source":["#### Treinamento de um classificador Naive Bayes\n","\n","Com a matriz construída para os dados de treinamento, vamos treinar um classificador utilizando o modelo MultinomialNaiveBayes()"]},{"metadata":{"id":"t2oJ8dE3h7k0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","modelo_NB = MultinomialNB()\n","modelo_NB.fit(X_train, train.target)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"y9EJcBZwh7k3","colab_type":"text"},"cell_type":"markdown","source":["#### Previsão\n","\n","Com os parâmetros do modelo já calculados, vamos fazer previsões sobre os dados de teste. \n","Como vetorizamos os dados de teste? Temos que usar aquele modelo tf-idf com que nós tínhamos transformado os dados de treinamento!\n","\n"]},{"metadata":{"id":"7600RlSKh7k4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"collapsed":true},"cell_type":"code","source":["X_test = modelo_tfidf.transform(test.data)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JtcaBGwYh7k7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Também é gerada uma matriz esparsa\n","type(X_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5SwxqGlrh7k-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"collapsed":true},"cell_type":"code","source":["# Com as features de teste transformadas calculamos as labels previstas\n","labels_predichas = modelo_NB.predict(X_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XyGWBOx7h7lC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["labels_predichas"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tc8Fjn0nh7lF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# As labels estão ligadas às tags del objeto Bunch original\n","train.target_names"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mVy87reGh7lJ","colab_type":"text"},"cell_type":"markdown","source":["### Matriz de confusão\n","\n","A seguir, criamos o gráfico de una matriz de confusão entre a previsão e as verdadeiras labels em teste"]},{"metadata":{"id":"B26ff7Qoh7lJ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","% matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rLpZCK-uh7lN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Primeiro, calculamos a accuracy geral do modelo\n","from sklearn.metrics import accuracy_score\n","accuracy_score(test.target, labels_predichas)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"w4VoLYq6h7lR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Imprimimos a matriz de confusão\n","mat = confusion_matrix(test.target, labels_predichas)\n","mat.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FqqB60tuh7lU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n","xticklabels=train.target_names, yticklabels=train.target_names)plt.xlabel('Etiquetas verdaderas')\n","plt.xlabel('Etiquetas predichas');"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6UH5KvAzh7lY","colab_type":"text"},"cell_type":"markdown","source":["#### Conclusão\n","\n","Mesmo com um modelo muito simples como o Naive Bayes, nós podemos classificar um corpus de texto com bastante precisão.\n","\n","Na matriz de confusão podemos observar algo que era esperável: os correios sobre religião em geral se confundem com aqueles específicos sobre a religião cristã, já que empregam um vocabulário semelhante."]},{"metadata":{"id":"wXF9zTlwh7lb","colab_type":"text"},"cell_type":"markdown","source":["Um ponto interessante a considerar é que agora temos um classificador de strings. Podemos colocar qualquer string nele para classificá-la de forma automática. Naturalmente, vamos ter que fazer isso em inglês. Somente isolemos tudo em uma única função:"]},{"metadata":{"id":"weBpDwMjh7lc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"collapsed":true},"cell_type":"code","source":["def predict_category(s,train=train):\n","    x = modelo_tfidf.transform([s]) # chamamos o vetorizador que usamos (tem que ser o mesmo)\n","    pred = modelo_NB.predict(x) # fazemos o predict, usando o modelo que fitamos.\n","    return train.target_names[pred[0]] # retornamos a categoria correspondente à previsão"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CdS0uhOnh7le","colab_type":"text"},"cell_type":"markdown","source":["Vamos provar:"]},{"metadata":{"id":"8BDuSJ8qh7lg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["predict_category('sending a payload to the ISS')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5RdyifjSh7li","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["predict_category('discussing islam vs atheism')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WIilsbMfh7lk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["predict_category('determining the screen resolution')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6ox4MZQEh7ln","colab_type":"text"},"cell_type":"markdown","source":["É preciso lembrar que isso não tem qualquer sofisticação: trata-se simplesmente de um modelo probabilístico (de hipóteses bastante simples) para a frequência ponderada de cada palavra em uma string. Contudo, obteremos um nível de accuracy bastante razoável e resultados bastante efetivos."]},{"metadata":{"id":"IbPprfr0h7ln","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"collapsed":true},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}