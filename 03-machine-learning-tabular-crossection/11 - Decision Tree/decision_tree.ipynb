{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árvores de Decisão\n",
    "\n",
    "\n",
    "## O que são?\n",
    "Árvores de decisão são métodos de aprendizado de máquinas supervisionado não-paramétricos, muito utilizados em tarefas de classificação e regressão.\n",
    "\n",
    "WTF?!?! Muita calma nessa hora. Antes de explicarmos melhor o significado dessa definição, vamos entender o que é uma árvore de decisão a partir de uma perspectiva mais geral.\n",
    "\n",
    "Provavelmente, você já deve ter visto ou utilizado uma árvore de decisão em sua vida. Por exemplo, você já deve ter utilizado ou visto um fluxograma, não é mesmo? Espero que sim (senão dê uma olhada na figura abaixo). Isso mesmo! Um fluxograma pode ser considerado uma árvore de decisão (caso não contenha loop).\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/0*Wy3QjtXL9qf-Ssyz.jpg'>\n",
    "\n",
    "\n",
    "Árvores, de um modo geral em computação, são estruturas de dados formadas por um conjunto de elementos que armazenam informações chamadas nós. Na figura acima, os nós são representados pelos quadradinhos com as perguntas e as informações podem ser consideradas as perguntas e suas possíveis respostas. Além disso, toda árvore possui um nó chamado raiz, que possui o maior nível hierárquico (o ponto de partida) e ligações para outros elementos, denominados filhos. Esses filhos podem possuir seus próprios filhos que por sua vez também possuem os seus. O nó que não possui filho é conhecido como nó folha ou terminal (representado pelo símbolo arredondado na figura). Tendo essas definições esclarecidas, uma árvore de decisão nada mais é que uma árvore que armazena regras em seus nós, e os nós folhas representam a decisão a ser tomada (no caso do exemplo, qual jogo escolher).\n",
    "\n",
    "Em uma árvore de decisão, uma decisão é tomada através do caminhamento a partir do nó raiz até o nó folha. Para elucidar melhor, suponha que tenhamos essa situação: você vai fazer uma reunião com seus amigos em sua casa, e vai rolar comidas e bebidas. Você quer jogar um jogo de tabuleiro, mas não sabe qual o melhor para essa situação, portanto, você vai recorrer a árvore de decisão para tomar sua decisão de forma mais correta. Para isso vamos caminhar por ela a partir do nó raiz, que contém a pergunta: Está jogando com crianças? Não, então iremos para nó filho da esquerda. Vamos jogar por mais de duas horas? Acho que sim! Assim, o próximo é: Regras difíceis? Vou tomar uma, logo não vou querer pensar muito rsrs. Reposta é não. Todos os jogadores vão ficar até o final? Vamos supor que sim! Desse modo, chegamos ao nó folha, com jogo Le Havre.\n",
    "\n",
    "Simples assim! Mas o que isso tem a ver com aprendizado de máquina? De fato, uma árvore de decisão por si só não é aprendizado de máquina já que eu posso criar uma sem auxílio de um computador e utilizá-la para organizar melhor minhas ideias e tomada de decisão (como alguém fez com o problema de escolher jogos de tabuleiro). Todavia, seu processo de construção automático, a partir de um conjunto de dados, é. No geral, os algoritmos para isso o fazem de forma não-paramétrica (termo estatístico que significa, em termos gerais: não assumir nada sobre os dados de antemão) e supervisionada (os dados devem conter as respostas/rótulos, mais detalhes aqui).\n",
    "\n",
    "Tendo tudo isso em vista, na próxima seção veremos como aprender a estrutura da árvore de decisão de forma automática a partir dos dados.\n",
    "\n",
    "\n",
    "## Então diga-me, como construo uma?\n",
    "A ideia geral de métodos baseados em árvores é particionar o espaço recursivamente em retângulos (sub-regiões), nos quais um modelo simples é aprendido.\n",
    "\n",
    "Para elucidar melhor como a árvore de decisão particiona o espaço em sub-regiões, vamos introduzir o conjunto de dados de flores de íris.\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/0*yYe75ni14sly7gYo.'>\n",
    "\n",
    "Esse conjunto de dados contém exemplos de três espécies de flor de Íris. Cada um dos quais possui 4 atributos, que são a largura e comprimento da pétala e da sépala da flor. \n",
    "Nós iremos utilizar apenas 2 atributos em nosso exemplo (comprimento e largura da pétala), já que é mais fácil para visualização no plano 2d. Além disso, conseguimos dividir o espaço satisfatoriamente apenas com esses dois atributos, ou seja, são dois atributos bem discriminativos para esse problema.\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1200/0*VyeowlJuw5gXYRer.'>\n",
    "\n",
    "O espaço é representado pelo gráfico ao lado, onde cada uma das três cores representa uma espécie da flor de íris, são elas: setosa (amarela), versicolor (verde) e viriginica (roxa). Tendo isso em vista, nos queremos construir uma árvore de decisão para determinar a qual espécie uma nova flor de íris pertence dada suas características (nesse caso, comprimento e largura da pétala). Para isso, vamos particionar o espaço recursivamente, fazendo cortes ortogonais, levando em consideração uma variável por vez, que maximiza a pureza das sub-regiões resultantes (pureza = homogeneidade no que diz respeito às espécies/rótulos). Cada divisão do espaço é representada por um nó na árvore de decisão. A primeira divisão (nós raiz da árvore) leva em consideração todos os exemplos (pontos) do espaço ao encontrar o ponto de corte que maximiza a pureza, de acordo com algum critério de impureza (veremos adiante), das sub-regiões resultantes. Vamos supor que o comprimento da pétala de valor 2,45 cm é o ponto de corte que melhor divide o espaço. Portanto, essa será o nó raiz de nossa árvore de decisão.\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/1*HMcpfu9zlpTurI2h2mugCQ.png'>\n",
    "\n",
    "A figura acima mostra como fica essa divisão (esquerda) e nosso nó raiz (direita). De fato, esse corte nos deu uma região totalmente pura (conseguimos separa as setosas do restante) e uma outra heterogênea. Agora, vamos recursivamente particionar as duas sub-regiões do espaço (da esquerda e da direita). Na esquerda, temos uma região completamente pura e isso significa que qualquer corte que fizermos nessa região teremos como resultados regiões puras. Portanto, podemos parar o particionamento, ou seja, essa região será considerada um nó folha em nossa árvore. Já a região da direita não é pura, assim devemos continuar o particionamento por ela até que cheguemos em sub-regiões puras. Figura a seguir mostra a próxima partição.\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/1*0iIETN4VtYOFke9MK1zi-g.png'>\n",
    "\n",
    "A partição na sub-região da direita, foi feita no eixo da largura da pétala no valor 1,75 cm. Como pode-se notar, foram gerados mais duas novas sub-regiões disjuntas, que, por sua vez, não estão completamente puras. Esse processo é repetido recursivamente até que um critério de parada seja alcançado (i.e., profundidade, número de folhas) ou que todas as folhas sejam puras, que implica na costrução até sua profundidade máxima . A figura a seguir mostra como ficaria nossa árvore e as partições (quase) puras do espaço.\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/1*fXpkqRhmvuJq7SvK7XTknw.png'>\n",
    "\n",
    "Essa é a intuição do algoritmo de construção de uma árvore de decisão. Há vários algoritmos para aprender sua estrutura, são eles CART, ID3 e C4.5, todos seguem mais ou menos essa lógica. Nesse artigo, vamos focar no algoritmo CART, pois é um dos mais intuitivos e simples de implementar, além de sua implementação estar disponível em várias bibliotecas de aprendizado de máquinas, tal como scikit-learn. O código a seguir sintetiza o algoritmo.\n",
    "\n",
    "\n",
    "# Como o algoritmo funciona?\n",
    "\n",
    "<img src='https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1545934190/2_btay8n.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(data, labels, tree, depth = 1):\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    n_classes = classes.shape[0]\n",
    "\n",
    "    # criétio de parada\n",
    "    if not stopping_criterion(n_classes, depth, tree.max_depth):\n",
    "        node = Node()\n",
    "\n",
    "        # encontra melhor ponto de corte dado a região atual do espaço\n",
    "        # de acordo com critério de impureza escolhido\n",
    "        feature, threshold = find_cut_point(data, labels, \n",
    "                                            tree.impurity_criterion)\n",
    "        \n",
    "        # aplicando o limiar para particionar o espaço\n",
    "        mask = data[:, feature] <= threshold\n",
    "        \n",
    "        # contruindo árvore recursivamente para\n",
    "        # os sub-espaço da direita e da esquerda.\n",
    "        left = build_tree(data[mask], labels[mask], tree, depth + 1)\n",
    "        right = build_tree(data[~mask], labels[~mask], tree, depth + 1)\n",
    "     \n",
    "        return Node(feature=feature, threshold=threshold, left=left, right=right)\n",
    "\n",
    "    # calcula a quantidade de exemplos por classe nesse nó folha\n",
    "    # e instancia um nó folha com essas quantidades, lembre-se que isso\n",
    "    # será usado para predição. \n",
    "    values = np.zeros(tree.n_classes)\n",
    "    values[classes] = counts\n",
    "    return Node(is_leaf=True, counts=values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encontrando \"melhor\" ponto de corte\n",
    "Nós estamos interessados em encontrar a melhor divisão em termos de alguma medida de impureza, tal como Gini Index, Entropia ou taxa de erro (detalharemos mais adiante). Encontrar o ponto de corte que leva a árvore de decisão ótima pode ser computacionalmente inviável (construir a árvore de decisão binária ótima é um problema np-completo). Portanto, nós nos contentamos em fazer escolhas locais ótimas (não garantindo o ótimo global) para encontrar o melhor atributo e limiar para o particionamento (linha 7 do código acima).\n",
    "\n",
    "Para encontrar melhor ponto de corte, nós temos de testar todos os possíveis, ou seja, para cada atributo e valores possíveis calcular o ganho de informação (quão pura a divisão torna o espaço) para cada um dos pontos de corte candidatos. Após essa etapa, nós escolhemos o candidato com maior ganho de informação para ser o ponto de corte do nó em questão. O código para isso ficaria o seguinte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cut_point(data, labels, impurity_criterion = gini_criterion):\n",
    "  \"\"\" find the best cut point \n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  data: numpy array-like = [n_samples, n_features]\n",
    "  labels: numpy array-like, shape = [n_samples]\n",
    "  impurity_criterion: callable, default=gini_criterion\n",
    "  \n",
    "  Return\n",
    "  ------\n",
    "  feature, threshold\n",
    "  \"\"\"\n",
    "  n_samples, n_features = data.shape\n",
    "\n",
    "  max_info_gain = np.iinfo(np.int32).min\n",
    "  feat_id = 0\n",
    "  best_threshold = 0\n",
    "\n",
    "  # pré-calculando a impureza da região atual\n",
    "  H_parent = impurity_criterion(data, labels)\n",
    "  # para cada um dos atributos\n",
    "  # vamos tentar encontrar o limiar que maximiza o ganho de informação\n",
    "  for j in range(n_features):\n",
    "    # só nos interessa os valores ordenados únicos \n",
    "    # do atributo j nessa região do espaço\n",
    "    values = np.unique(data[:, j])\n",
    "    \n",
    "    for i in range(values.shape[0] - 1):\n",
    "      # usamos o ponto médio dos valores possíveis\n",
    "      # como limiar candidato para o ponto de corte\n",
    "      threshold = (values[i] + values[i + 1]) / 2.\n",
    "\n",
    "      mask = data[:, j] <= threshold\n",
    "\n",
    "      info_gain = H_parent \\\n",
    "                  - (mask.sum() * impurity_criterion(data[mask], labels[mask]) \\\n",
    "                  + (~mask).sum() * impurity_criterion(data[~mask], labels[~mask])) \\\n",
    "                  / float(n_samples)\n",
    "\n",
    "      if max_info_gain < info_gain:\n",
    "        best_threshold = threshold\n",
    "        feat_id = j\n",
    "        max_info_gain = info_gain\n",
    "        \n",
    "  return feat_id, best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculando o ganho de informação\n",
    "O ganho de informação, de grosso modo, representa a informação aprendida sobre os rótulos (no nosso exemplo, as espécies) quando dividimos uma região do espaço em duas sub-regiões de acordo com ponto de corte. E é definida pela fórmula:\n",
    "\n",
    "$$InfoGain(R, R_e, R_d) = H(R) - (|R_e|*H(R_e) + |R_d|*H(R_d)) / |R|,$$\n",
    "\n",
    "onde H é a impureza da região, R é a região atual, R_e é sub-região da esquerda, R_d é sub-região da direita e |.| é quantidade de exemplos na dada região. Os critérios de impureza mais comuns para classificação são a Entropia e o índice Gini, definidos a seguir:\n",
    "\n",
    "$$entropia(R) = -∑ p(c|R) log (p(c|R)) ,$$\n",
    "\n",
    "$$gini(R) = ∑ p(c|R) (1 - p(c|R)),$$\n",
    "\n",
    "onde p(c|R) é a probabilidade de um ponto da região R pertencer a classe c. Essa probabilidade é estimada pela razão entre quantidade de pontos da classe c e o total de pontos em R.\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/1*HMcpfu9zlpTurI2h2mugCQ.png'>\n",
    "\n",
    "Para ficar mais claro, vamos calcular o ganho de informação para o ponto de corte mostrado na figura utilizando tanto a entropia quanto o gini. Vamos iniciar calculando p(c|R) para cada região e espécie.\n",
    "\n",
    "Sabendo que temos 50 exemplos de cada espécie, a p(c|R) para todas as espécies é 50 / 150 ~ 0.33 . Calculando para a sub-região da esquerda, temos que p(c = setosa | R_e) = 1.0 (só temos setosa nessa região) e p(c = versicolor | R_e) = p(c = virginica | R_e) = 0.0. Para direita, p(c = setosa | R_d) = 0.0 e p(c = versicolor | R_d) = p(c = virginica | R_d) = 0.5.\n",
    "\n",
    "Com esses valores em mãos, fica fácil calcular a entropia e gini de cada região e, por consequência, obter o ganho de informação.\n",
    "\n",
    "$$ entropia(R) = -∑ p(c|R) log (p(c|R)) =- 3 * (0.33 log(0.33)) ~0.48$$\n",
    "\n",
    "$$entropia(R_e) =-(1.0 log(1.0) + 0.0 log(0.0) + 0.0 log(0.0)*) = 0$$\n",
    "\n",
    "$$entropia(R_d) =-(0.0 log(0.0) + 0.5 log(0.5) + 0.5 log(0.5)*) ~0.30$$\n",
    "\n",
    "Portanto, o ganho de informação usando entropia como critério de impureza é:\n",
    "\n",
    "$$InfoGain = 0.48 - (50*0 + 100*0.30) / 150 = 0.28$$\n",
    "\n",
    "Calculando o gini de maneira análoga,\n",
    "\n",
    "\n",
    "\n",
    "$$gini(R) = ∑ p(c|R) (1 - p(c|R)) = 3 * (0.33 *(1 - 0.33)) ~0.66$$\n",
    "\n",
    "$$gini(R_e) =(1.0 * (1.0 - 1.0) + 0.0 * (1 - 0.0) + 0.0 * (1 - 0.0)) = 0$$\n",
    "\n",
    "$$gini(R_d) =(0.0 * (1 - 0.0) + 0.5 * (1 - 0.5) + 0.5 * (1 - 0.5)) ~0.5$$\n",
    "\n",
    "\n",
    "Portanto, o ganho de informação usando gini como critério de impureza é:\n",
    "\n",
    "$$ InfoGain = 0.66 - (50*0 + 100*0.50) / 150 = 0.16 $$\n",
    "\n",
    "As implementações desses critérios de impureza seriam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_criterion(data, labels):\n",
    "  \"\"\" Entropy\n",
    "  Parameters\n",
    "  ----------\n",
    "  data: numpy array-like = [n_samples, n_features]\n",
    "  labels: numpy array-like, shape = [n_samples]\n",
    "  \n",
    "  Return\n",
    "  ------\n",
    "  entropy: float\n",
    "  \"\"\"\n",
    "  classes = np.unique(labels)\n",
    "  \n",
    "  s = 0\n",
    "  for c in classes:\n",
    "    p = np.mean(labels == c)\n",
    "    s -= p * np.log(p)\n",
    "    \n",
    "  return s\n",
    "  \n",
    "\n",
    "def gini_criterion(data, labels):\n",
    "  \"\"\" Gini Index\n",
    "  Parameters\n",
    "  ----------\n",
    "  data: numpy array-like = [n_samples, n_features]\n",
    "  labels: numpy array-like, shape = [n_samples]\n",
    "  \n",
    "  Return\n",
    "  ------\n",
    "  gini: float\n",
    "  \"\"\"\n",
    "  classes = np.unique(labels)\n",
    "  \n",
    "  s = 0\n",
    "  for c in classes:\n",
    "    p = np.mean(labels == c)\n",
    "    s += p * (1 - p)\n",
    "    \n",
    "  return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo está praticamente finalizado, só precisamos definir nosso critério de parada e pronto (essa é parte mais fácil).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopping_criterion(n_classes, depth, max_depth):\n",
    "  \"\"\" Stopping criterion\n",
    "  Parameters\n",
    "  ----------\n",
    "  n_classe: int\n",
    "            number of classes in the region, one means that the region is pure.\n",
    "  depth: int,\n",
    "          current tree depth\n",
    "  max_depth: int, default=None\n",
    "          maximal tree depth. None for fully grown tree.\n",
    "  Return\n",
    "  ------\n",
    "  bool\n",
    "  \"\"\"\n",
    "  return (max_depth is not None and max_depth == depth) or (n_classes == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traduzindo o código, nosso critério é verdadeiro se atingir uma profundidade máxima preestabelecida ou se a região é pura (caso max_depth seja None a árvore crescerá até sua profundidade máxima). O código completo pode ser encontrado no link a seguir:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Por que árvores de decisão são tão populares?\n",
    "Como pode-se ver, árvores de decisões são conceitualmente simples, porém poderosas. Sua popularidade é, principalmente, devido a suas características singulares:\n",
    "\n",
    "Fácil explicabilidade e interpretação, já que podemos facilmente visualizá-las (quando não são muito profundas).\n",
    "Requerem pouco esforço na preparação dos dados, métodos baseados em árvores normalmente não requerem normalização dos dados. Além disso, conseguem lidar com valores faltantes, categóricos e numéricos (não é o caso da CART que implementamos).\n",
    "Complexidade logarítmica na etapa de predição.\n",
    "São capazes de lidar com problemas com múltiplos rótulos.\n",
    "As árvores de decisão muito populares para problemas de classificação, regressão, análise exploratória entre outras tarefas.\n",
    "\n",
    "\n",
    "# Nem tudo são flores!\n",
    "Árvores de decisão possuem alguns probleminhas que podem degradar seu poder preditivo, são eles:\n",
    "\n",
    "Árvore crescida até sua profundidade máxima pode decorar o conjunto de treino (o temido overfitting), o que pode degradar seu poder preditivo quando aplicado a novos dados. Isso pode ser mitigado \"podando\" a árvore de decisão ao atribuir uma profundidade máxima ou uma quantidade máxima de folhas.\n",
    "São modelos instáveis (alta variância), pequena variações nos dados de treino podem resultar em árvores completamente distintas. Isso pode ser evitado ao treinarmos várias árvores distintas e agregar suas predições (que veremos em outro post).\n",
    "Como vimos, o algoritmo de construção da árvore de decisão é guloso, ou seja, não garante a construção da melhor estrutura para o dados de treino em questão. Esse problema também pode ser mitigado ao treinarmos várias árvores distintas e agregar suas predições (veremos em outro post como fazer).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/uciml/pima-indians-diabetes-database/downloads/pima-indians-diabetes-database.zip/1\n",
    "\n",
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "# load dataset\n",
    "pima = pd.read_csv(\"diabetes.csv\", header=None, names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n",
    "pima.drop(0, inplace=True) # remove first row\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima.label # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datascience.stackexchange.com/questions/37428/graphviz-and-pydotplus-not-working\n",
    "# !conda install python-graphviz\n",
    "# !pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('diabetes.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fontes:\n",
    "- https://medium.com/machine-learning-beyond-deep-learning/%C3%A1rvores-de-decis%C3%A3o-3f52f6420b69\n",
    "- https://www.datacamp.com/community/tutorials/decision-tree-classification-python        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
